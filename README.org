# -*- org -*-
#+TITLE: 机器学习笔记
#+AUTHOR: GoldenRaven
#+DATE: <2020-02-27 Thu>
#+email: li.gaoyang@live.com
#+OPTIONS: num:nil

* 目录                                                         :TOC@2:QUOTE:
#+BEGIN_QUOTE
- [[#pandas笔记][Pandas笔记]]
  - [[#series][Series]]
  - [[#dataframe][DataFrame]]
- [[#numpy笔记][Numpy笔记]]
  - [[#copy-or-view][Copy or View?]]
  - [[#shallow-copy-vs-deep-copy][Shallow Copy vs. Deep Copy?]]
  - [[#常用函数][常用函数]]
- [[#数据集操作的一般过程以kaggle-titanic为例][数据集操作的一般过程（以Kaggle Titanic为例）]]
  - [[#加载数据][加载数据]]
  - [[#预览数据][预览数据]]
  - [[#清理数据][清理数据]]
  - [[#从训练集中分离出测试集][从训练集中分离出测试集]]
  - [[#分析数据找到关联性][分析数据，找到关联性]]
  - [[#模型化数据集][模型化数据集]]
- [[#注意点摘自机器学习实战][注意点（摘自机器学习实战）]]
#+END_QUOTE

* Pandas笔记
** Series
#+begin_src python
pd.Series
#+end_src
** DataFrame
- DataFrame中的方法属性：
#+NAME: df_methods
#+BEGIN_src example
'T', 'abs', 'add', 'add_prefix', 'add_suffix', 'agg', 'aggregate', 'align', 'all',
'any', 'append', 'apply', 'applymap', 'asfreq', 'asof', 'assign', 'astype',
'at', 'at_time', 'attrs', 'axes', 'between_time', 'bfill', 'bool', 'boxplot',
'clip', 'columns', 'combine', 'combine_first', 'convert_dtypes', 'copy', 'corr',
'corrwith', 'count', 'cov', 'cummax', 'cummin', 'cumprod', 'cumsum', 'describe',
'diff', 'div', 'divide', 'dot', 'drop', 'drop_duplicates', 'droplevel', 'dropna',
'dtypes', 'duplicated', 'empty', 'eq', 'equals', 'eval', 'ewm', 'expanding',
'explode', 'ffill', 'fillna', 'filter', 'first', 'first_valid_index', 'floordiv',
'from_dict', 'from_records', 'ge', 'get', 'groupby', 'gt', 'head', 'hist', 'iat',
'idxmax', 'idxmin', 'iloc', 'index', 'infer_objects', 'info', 'insert',
'interpolate', 'isin', 'isna', 'isnull', 'items', 'iteritems', 'iterrows',
'itertuples', 'join', 'keys', 'kurt', 'kurtosis', 'last', 'last_valid_index',
'le', 'loc','lookup', 'lt', 'mad', 'mask', 'max', 'mean', 'median', 'melt',
'memory_usage', 'merge', 'min', 'mod', 'mode', 'mul', 'multiply', 'ndim', 'ne',
'nlargest', 'notna', 'notnull', 'nsmallest', 'nunique', 'pct_change', 'pipe',
'pivot', 'pivot_table', 'plot', 'pop', 'pow', 'prod', 'product', 'quantile',
'query', 'radd', 'rank', 'rdiv', 'reindex', 'reindex_like', 'rename',
'rename_axis', 'reorder_levels', 'replace', 'resample','reset_index', 'rfloordiv',
'rmod', 'rmul', 'rolling', 'round', 'rpow', 'rsub', 'rtruediv', 'sample',
'select_dtypes', 'sem', 'set_axis', 'set_index', 'shape', 'shift', 'size',
'skew', 'slice_shift', 'sort_index', 'sort_values', 'sparse', 'squeeze', 'stack',
'std', 'style', 'sub', 'subtract', 'sum', 'swapaxes', 'swaplevel', 'tail', 'take',
'to_clipboard', 'to_csv', 'to_dict', 'to_excel', 'to_feather', 'to_gbq', 'to_hdf',
'to_html', 'to_json', 'to_latex', 'to_markdown', 'to_numpy', 'to_parquet',
'to_period', 'to_pickle', 'to_records', 'to_sql', 'to_stata', 'to_string',
'to_timestamp', 'to_xarray', 'transform', 'transpose', 'truediv', 'truncate',
'tshift', 'tz_convert', 'tz_localize', 'unstack', 'update', 'values', 'var',
'where', 'xs'
#+END_SRC

* Numpy笔记
** Copy or View?
** Shallow Copy vs. Deep Copy?
** 常用函数
#+BEGIN_SRC python :results
import numpy as np
a = np.arange(3)
b = np.arange(3,6)
c = np.r_[a, b, 1, [3]] # 合并数组
d = np.c_[a, b] # 合并数组
e = np.ones((4, 1)) # 接收元组
d.shape
d.resize(2, 3) # 无返回值，将原数组形变，接收元组
f = d.reshape(（2,3）) # 返回变形后的数组，原数组不变，接收元组
#+END_SRC

* 数据集操作的一般过程（以Kaggle Titanic为例）
疑问：
- 分离出测试集应该在预览数据之前还是之后？
- 清理数据时，是对整个数据集操作还是只对训练集操作？
- 交叉验证总是需要的吗？只是在出现过拟合时使用？
Jupyter Notebook:
[[file:titanic-a_data_science_framework_to_achieve_99_accuracy.ipynb][titanic-a_data_science_framework_to_achieve_99_accuracy.ipynb]]
** 加载数据
Kaggle网站上Titanic竞赛中的数据集test.csv指求解时的实例，去掉了标签'Survived'.
#+NAME: read
#+BEGIN_src python
import pandas as pd
df = pd.read_csv("train.csv") # , delimiter=',')
df2 = pd.read_csv("test.csv") # , delimiter=',')
#+END_SRC
- 要注意Python中赋值时，引用与复制数据的区别！
#+NAME: cp
#+BEGIN_src python
data1 = df.copy(deep=False) #不复制df的indices和数据，只创建一个指向原数据的引用
data1 = df.copy(deep=True) #复制df的indices和数据，并在内存中创建新的对象
#+END_SRC
- 引用也是很有用的，尤其是在[[清理数据][清理数据]]时（为什么要清理data-val？）
#+NAME: celan_by_reference
#+BEGIN_src python
data_clearner = [data1, df2] #可以一起清理
#+END_SRC
** 预览数据
#+NAME: preview
#+BEGIN_src python
df.info()
df.head()
df.tail()
df.sample(10)
#+END_SRC
** 清理数据
4个'C':
- *Correcting*: 更正异常值，离群值
- *Completing*: 补全缺失信息
- *Creating*: 创建新的特征，用以之后的分析
- *Converting*: 转换数据的格式，以备后续的计算与呈现

*** 3.3.1 Completing
不推荐删除记录，尤其当它占的比例大时。最好impute. 对于定性值，
一般使用mode，对于定量值一般用中值、平均值或以平均值+随机化的标准差来代替。
还有针对具体问题更特殊的处理方法，如代之以某个小类别中的中值等。
#+BEGIN_src python
df.isna().sum() # 查看数据中的空值情况
df.isnull().sum() # 查看数据中的空值情况
df.describe(include='all') #数据的简单分析
df['Age'].fillna(df['Age'].median(), inplace=True) # 用中值来补全空值（定量值）
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)
# 用出现最的类别来补全空值（定性值）
drop_index = ['PassengerId', 'Ticket'] # index to drop
df.drop(drop_index, axis=1, inplace=True) # drop features/columns
#+END_SRC
*** 3.3.2 Creating
特征工程：用已经存在的特征来创造新的特征，以检查是否对结果预测提供新的信息。
#+BEGIN_src python
df['FamilySize'] = df.['SibSp'] + df.['Parch'] + 1 # 新建特征
df['Alone'] = 0
df['Alone'].loc[df['FamilySize'] > 1] = 1 # 选择性赋值
df['Title'] = df['Name'].str.split( # 特征中字符串截取
   ', ', expand=True)[1].str.split('.', expand=True)[0]
df['FareBins'] = pd.cut(df['Fare'], 4) # 离散化连续值到区间
df['AgeBins'] = pd.qcut(df['Age'].astype(int), 5) # 离散化连续值到区间
# 清理类别数太少的类别
title_name = df['Title'].value_counts() < 10
df['Title'] = df['Title'].apply(lambda x: 'Misc' if title_name[x] else x)
#+END_SRC
*** 3.3.3 Converting
如对object类型的数据格式化，使算法可以处理。
#+BEGIN_src python
from sklearn.preprocessing import LabelEncoder OneHotEncoder
# 数字型编码
encoder = LabelEncoder()
df['Sex']  = encoder.fit_transform(df['Sex'])
# 独热向量编码， 接收二维数组
encoder2 = OneHotEncoder()
df['Sex']  = encoder2.fit_transform(df['Sex'].reshape(-1,1))
#+END_SRC
** 从训练集中分离出测试集
两种方法：
- 固定比例分离
# #+BEGIN_src example
# model_selection.train_test_split(X, y, test_size=0.4, random_state=42)
# #+END_SRC
#+BEGIN_SRC python
train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)
#+END_SRC
- 交叉验证（用以比较模型）
#+BEGIN_src example
model_selection.cross_val_score()
#+END_SRC
** 分析数据，找到关联性
#+BEGIN_SRC python
df[['Sex', 'Survived']].groupby('Sex',as_index=False).mean() # 特定特征与标签的关系
# 图示某一个特征与标签的关系
plt.hist(x = [df[df['Survived']==1]['Fare'], df[df['Survived']==0]['Fare']],
         stacked=True, color = ['g','r'],label = ['Survived','Dead'])
plt.title('Fare Histogram by Survival')
plt.xlabel('Fare ($)')
plt.ylabel('# of Passengers')
plt.legend()
# 图示某两个特征与标签的关系
sns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)
axis2.set_title('Pclass vs Age Survival Comparison')
# Pearson关联
df.corr()
#+END_SRC

Pearson关联的绘图函数：
#+BEGIN_SRC python
#correlation heatmap of dataset
def correlation_heatmap(df):
    _ , ax = plt.subplots(figsize =(14, 12))
    colormap = sns.diverging_palette(220, 10, as_cmap = True)

    _ = sns.heatmap(
        df.corr(),
        cmap = colormap,
        square=True,
        cbar_kws={'shrink':.9 },
        ax=ax,
        annot=True,
        linewidths=0.1,vmax=1.0, linecolor='white',
        annot_kws={'fontsize':12 }
    )

    plt.title('Pearson Correlation of Features', y=1.05, size=15)

correlation_heatmap(data1)
#+end_src
** 模型化数据集
*** 背景知识
机器学习算法可以分为四个部分：
- 分类
- 回归
- 聚类
- 降维
机器学习知识：
- [[https://scikit-learn.org/stable/user_guide.html][Sklearn Estimator Overview]]
- [[https://scikit-learn.org/stable/modules/classes.html][Sklearn Estimator Detail]]
- [[https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html][Choosing Estimator Mind Map]]
#+attr_html: :width 900px
#+attr_latex: :width 900px
#+attr_org: :width 900px
[[file:images/sklearn_mindmap.png]]
- [[https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf][Choosing Estimator Cheat Sheet]]
[[file:images/cheatsheet.png]]
解决有监督的分类问题的算法：
- Ensemble Methods
- Generalized Linear Models (GLM)
- Naive Bayes
- Nearest Neighbors
- Support Vector Machines (SVM)
- Decision Trees
- Discriminant Analysis
* 注意点（摘自机器学习实战）
- 对收入分层抽样，不能分太多层
- 分层方法：除以1.5，向上取整；然后合并大于5的分类
- 地理数据可视化，用其他相关属性作为颜色，和散点大小
- 寻找与标签相关性高的属性，用df.corr()['labels']
- 进一步考察高相关性属性的数据模式，并删除可能的错误数据
- 尝试不同的属性组合，以找到高相关性特征
- 将预测器与标签分离，因为可能不一定对它们使用相同的转换方式
- 特征缩放（归一化、标准化），即同比缩放所有属性
- 评估训练得的模型，对训练集求RMSE或MAE
- 误差较大则拟合不足，可以
- 误差过小？则用验证集来验证得到的模型，以检查是否过拟合
- 交叉验证，可以sklearn的K-fold功能
- 如果在验证集上得到的误差大则说明确实有过拟合，需要更换模型
- 尝试多个模型以找到2-5个有效的模型，别花太多时间去调整超参数
- 保存每个尝试过的模型，用pickel或sklearn的joblib
- 训练集分数明显低于验证集分数，则过度拟合
- 注意：目标值一般不进行绽放，并且只对训练集缩放
