* 第四章和第五章总结
今天总结了一下《机器学习实战》第四章和第五章的内容和sklearn的user guide相关
内容。通过算法之间的横向比较，对目前尝到的回归算法和分类算法有了更深刻地理解。但
总感觉对支持向量机的工作机制还是理解不够，书上内容过于简略，需要找其他的书或网页
来看看。

#+BEGIN_QUOTE
只读一本书不是学习，是娱乐。
#+END_QUOTE

为了做预测， 我们只需要看 f(x) 的符号。找到模型参数的一般选择是通过最小化由以下式子给出的正则化训练误差。
[[file:images/580270908cf4e5ba3907b7267fcfbb44.jpg]]

其中 L 衡量模型(mis)拟合程度的损失函数，R 是惩罚模型复杂度的正则化项（也叫作惩罚）;
 \alpha > 0是一个超参数。

不同的算法从不同的成本函数或损失函数出发，再加上不同的正则化项就形成了不同
算法。sklearn对这些算法都有实现，不同的实现适用的问题也不同，具体见表格。

** 回归算法
目前学习到的用于 ~回归任务~ 的算法包括：
- 线性回归（不加正则化的线性回归，或普通的最小二乘法，LinearRegression）
- 岭回归（Ridge）
- 套索回归（Lasso）
- 弹性网络（ElasticNet）
- 线性支持向量机回归器（LinearSVR）
- 支持向量机回归器（SVR）
- 随机梯度下降回归器（SGDRegressor）

sklearn实现的不同回归算法的横向对比见下表：

| sklearn类名      | 损失函数(loss)              | 正则项(penalty)                        | 模型名称           | 是否支持核技巧 | 优点                                                                                 | 缺点                                    |
|------------------+-----------------------------+----------------------------------------+--------------------+----------------+--------------------------------------------------------------------------------------+-----------------------------------------|
| LinearRegression | squared_loss                | 无                                     | 普通最小二乘       | 否             | 使用 *X* 矩阵的SVD分解（闭式解），直接求解，不用迭代                                 | 大数据集太慢，特征相关性强时误差很大    |
| Ridge            | squared_loss                | l_{2}正则项                            | 岭回归             | 否             | 闭式解法，同上;比普通最小二乘法稳定                                                  | 大数据集太慢                            |
| Lasso            | squared_loss                | l_{1}正则项                            | 套索回归           | 否             | 实现方法：坐标下降（迭代优化）;可用于特征选择                                        | 当n>m或特征强相关时可能非常不稳定       |
| ElasticNet       | squared_loss                | \rho l_{1}+(1-\rho)l_{2}               | 弹性网络           | 否             | 继承了Ridge的稳定性，也可作特征选择                                                  | 大数据集太慢                            |
| SGDRegressor     | squared_loss                | {None, 'l_{2}', 'l_{1}', 'elasticnet'} | 以上四种算法       | 否             | 随机梯度下降（迭代优化），可用于大数据集（高效）                                     | 对缩放敏感，需要设置一些超参,对缩放敏感 |
|                  | huber                       | {None, 'l_{2}', 'l_{1}', 'elasticnet'} | Huber回归          | 否             | 同上？                                                                               | 同上？                                  |
|                  | epsilon_insensitive         | {None, 'l_{2}', 'l_{1}', 'elasticnet'} | 线性支持向量机回归 | 否             | 同上？                                                                               | 同上？                                  |
|                  | squared_epsilon_insensitive | {None, 'l_{2}', 'l_{1}', 'elasticnet'} | 无                 | 否             | 同上？                                                                               | 同上？                                  |
| LinearSVR        | epsilon_insensitive         | l_{1}正则项                            | 线性支持向量机回归 | 否             | 基于liblinear的优化算法，快速收敛 （迭代优化），在高维空间十分高效                   | 要小心过拟合，不提供概率估计,对缩放敏感 |
|                  | squared_epsilon_insensitive | l_{2}正则项                            | 线性支持向量机回归 | 否             | 同上？                                                                               | 同上？                                  |
| SVR              | epsilon_insensitive         | l_{2}正则项                            | 支持向量机回归     | 是             | kernel={linear, rbf, poly ,sigmoid ,precomputed ,自定义}可以用于线性、非线性回归任务 | 只适用于复杂但中小型的训练集,对缩放敏感 |

** 分类算法
目前学习到的用于 ~分类任务~ 的算法包括：
- 逻辑回归（LogisticRegression）
- 多元逻辑回归（Softmax回归）
- 线性支持向量机分类器（LinearSVC）
- 支持向量机分类器（SVC）
- 随机梯度下降分类器（SGDClassifier）

| sklearn类名        | 损失函数(loss)                                                  | 正则项(penalty)                        | 模型名称           | 优点                                                                                  | 缺点                                    |
|--------------------+-----------------------------------------------------------------+----------------------------------------+--------------------+---------------------------------------------------------------------------------------+-----------------------------------------|
| LogisticRegression | log                                                             | {None, 'l_{2}', 'l_{1}', 'elasticnet'} | 逻辑回归           | 求解器solver={newton-cg, lbfas, liblinear, sag, saga}, 可以实现多分类、多项式逻辑回归 | ？                                      |
| SGDClassifier      | hinge                                                           | {None, 'l_{2}', 'l_{1}', 'elasticnet'} | 线性支持向量机分类 | 高效，对大数据集有效                                                                  | 对缩放敏感                              |
|                    | log                                                             | 同上                                   | 逻辑回归           | 同上？                                                                                | 同上？                                  |
|                    | {modified_huber, squared_hinge, perceptron, 所有回归的损失函数} | 同上                                   | ？                 | 同上？                                                                                | 同上？                                  |
| LinearSVC          | {hinge, squared_hinge}                                          | {l_{2}, l_{1}}                         | 线性支持向量机分类 | 基于liblinear的优化算法，快速收敛 （迭代优化）                                        | 不支持核技巧,对缩放敏感                 |
| SVC                | hinge                                                           | l_{2}                                  | 支持向量机分类     | 支持核技术kernel={linear, rbf, poly, sigmoid, percomputed, 自定义}                    | 只适用于复杂但中小型的训练集,对缩放敏感 |

** 这么多算法该如何选择呢？
*经验：*
- 通常而言，有正则化总比没有强，所以大多数时候应该避免使用纯线性回归。岭回归是个不错的默认选择，
  但如果你觉得实际用到的特征只有少数几个，那就应该更倾向于Lasso或弹性网络，因为它们可以对特征进行自动选择。
  一般而言，弹性网络优于Lasso回归，因为当特征数大于训练实例数或特征强相关时，Lasso回归可能非常不稳定。
- 永远先从线性核函数开始尝试（记住：LinearSVC比SVC的linear kernel快得多），
  特别是当训练集很大或特征很多时。
- 如果训练不是很大，可以尝试高斯RBF核，大多数情况都很好用。
- 如果你还有多余时间和计算能力，可以使用交叉和网格搜索来尝试其他核函数，尤其是那些专门
  针对你的数据集数据结构的核函数（如字符串核）。
- 随机梯度下降法对 feature scaling （特征缩放）很敏感，因此强烈建议您缩放您的数据
- 最好使用 GridSearchCV 找到一个合理的 regularization term （正则化项） \alpha ，
  它的范围通常在 10.0**-np.arange(1,7)
- 经验表明，SGD 在处理约 10^6 训练样本后基本收敛。因此，对于迭代次数第一个合理的猜想是
  n_iter = np.ceil(10**6 / n)，其中 n 是训练集的大小。
- 我们发现，当特征很多或 eta0 很大时， ASGD（平均随机梯度下降） 效果更好。

#+BEGIN_QUOTE
疑问： one vs rest,  one vs one, one vs all, one vs any都是啥？都是不同的东西？
#+END_QUOTE

** 支持向量机SVM知识补充
*** 维基百科
[[https://zh.wikipedia.org/wiki/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA]]

支持向量机（support vector machine, 常简称SVM）是在分类与回归分析中分析数据的监督式学习模型与相关算法。
给定一个实例SVM将其标记为两个类别中的一个或另一个，使其成为一个二元线性分类器。

除了进行线性分类，SVM还可以通过 ~核技巧~ 将输入隐式地映射到高维空间，再应用线性模型，
从而有效地进行非线性分类。

此外SVM还可以用于聚类。

原始CVM算法由弗拉基米尔·万普尼克和亚历克塞·泽范兰杰斯于1963年发明的。1992年，Bernhard E. Boser、
Isabelle M. Guyon和弗拉基米尔·万普尼克提出了一种通过将核技巧应用于最大间隔超平面来创建非线性分类器的方法。
当前标准的前身（软间隔）由Corinna Cortes和Vapnik于1993年提出，并于1995年发表。

**** 线性SVM
- 硬间隔分类

如果训练数据是线性可分的，可以选择分离两类数据的两个平行超平面，使得它们之间的距离尽可能大，
以减小范化误差。可以得到这样一个优化问题：

[[file:images/hard-gap.png]]

一个显而易见却重要的结论是：最大间隔超平面完全由那些最靠近它的数据点所决定，
这些数据点被称为支持向量。

- 软间隔

为了将SVM扩展到线性不可分的数据集上，引入铰链损失函数（hinge）函数，
我们要最小化的函数变成：

[[file:images/soft-gap.png]]

**** 非线性分类（核技巧）
二阶多项式核：
\phi(*a*)^{T} \cdot \phi(*b*) = ((*a*)^{T} \cdot *b*)^{2}

**** 计算SVM分类器
硬间隔和软间隔都属于线性约束的凸二次优化问题。这类问题称为二次规划（QP）。
- 可以应用现成的二次规划的求解器。
- 也可以求解原问题（不能使用核技巧）的对偶问题（可以使用核技巧）
- 现代方法：次梯度下降（针对大训练集）和坐标下降（针对特征空间维度高）

**** 性质
SVM属于广义线性分类器一族，可以认为是感知器的延伸，也可以被认为是洪吉诺夫正则化特例。
SVM仅直接适用于两类任务。因此，必须应用将多类任务减少到几个二元问题的算法。
SVM算法最初是为二值分类问题设计的，实现多分类的主要方法是将一个多分类问题转化为多个二分类问题。
常见方法包括“一对多法”和“一对一法”，一对多法是将某个类别的样本归为一类,其他剩余的样本归为另一类，
这样k个类别的样本就构造出了k个二分类SVM；一对一法则是在任意两类样本之间设计一个SVM。
