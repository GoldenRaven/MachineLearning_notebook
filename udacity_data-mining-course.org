#+author: ligy
#+email: li.gaoyang@foxmail.com
#+date: <2020-04-07 Tue>

udacity的核心课程：数据挖掘工程师直通班，纳米学位。目标：在3到7天的时间里，完成所有数据挖掘课程，
同时还要进行项目练习。

总结：project1和project2的许多内容有所重复，project1以概括为主。project2更加深入，
但也不是很深入。虽然project2的许多函数我都用过，并且也可能使用地更细致，但这种课程形式
的讲解正是我所欠缺的。课程的学习可以将之前学会的零碎知识串联起来，对big picture有更好地
理解。可以消除只见树木不见森林的迷惘，与不踏实感，是很有必要的。主体结构有了之后，再自学就
更高效了。
* project1: 用python进行基础数据分析
** 数据分析的五个步骤
*** 提问
*** 整理数据wrangling
收集，评估，清理
**** 收集数据、评估数据
csv为comma separated values

#+BEGIN_SRC python :results output
import pandas as pd
df = pd.read_csv('file', header=1) # 使用第二行为标题，删除上面行的内容
df = pd.read_csv('student_scores.csv', header=None) # 没标题
labels = ['id', 'name', 'attendance', 'hw', 'test1',
'project1', 'test2', 'project2', 'final'] # 自定义标题
df = kkpd.read_csv('student_scores.csv', names=labels)
# 替换数据的标题行
df = pd.read_csv('student_scores.csv', header=0, names=labels)
#将一个或多个列指定为数据框的索引
df = pd.read_csv('student_scores.csv', index_col='Name')
#+END_SRC

#+RESULTS:
**** 清理数据
常见问题：类型错误、数据缺失、数据冗余（重复）、结构问题等。
#+BEGIN_SRC python :results output
# 用平均值填充
df['col_miss'].fillna(df['col_miss'].mean(), inplace=True)
# 查找数据重复情况
df.duplicated()
# 大数据集用
sum(df.duplicated())
# 去重复，查看更多参数
df.drop_duplicates(inplace=True)
# 将日期转成datatime类型。每次载入数据都需要转换！
df['dates'] = pd.to_datetime(df['dates'])
# 字符串是否被包含?
print('ac' in 'acbs')
# 为数据框中的列分配新标签
df.columns = new_labels
# 保存以后用
df.to_csv('new_file.csv', index=False)
#+END_SRC

*** 执行EDA（探索性数据分析）
DataFrame和Series对matplotlib有封装的绘图函数，但比较简单，复杂的还要用matplotlib.
#+BEGIN_SRC python :results output
# 在notebook中查看绘图
% matplotlib inline
# 用分号来隐藏不需要的输出
# hist绘直方图
df.hist(figsize=(10, 8));
# 也可以用在Series上
df['age'].hist();
# 统计值的出现次数，并绘图
df['age'].value_counts().plot(kind='bar') # 直方图
df['age'].value_counts().plot(kind='pie') # 饼状图
df.plot(x='age', y='height', kind='scatter')
df['age'].plot(kind='box'); #箱图
#+END_SRC

*** 得出结论
*** 传达结果
** Python包概述
包是函数和类等的集合，数据分析最常用的有Numpy, pandas, matplotlib
#+BEGIN_SRC python :results output
import numpy as np
l = list(range(9))
print(np.mean(l))
#+END_SRC

#+RESULTS:
: 4.0
* TODO project1的项目: Google Play Store Analysis
  - State "TODO"       from              [2020-04-09 Thu 12:23]
[[https://www.jianshu.com/p/04d180d90a3f][pandas入门]]

[[https://tonysyu.github.io/raw_content/matplotlib-style-gallery/gallery.html][图表呈现风格]]
* project2:数据整理
** lesson1：数据整理简介
最好用程序下载数据（API），而不是在网页直接下载。
*** 用python解压缩文件
zipfile是个上下文管理器，支持with语句。
#+BEGIN_SRC python :results output
import zipfile
with zipfile.ZipFile('file.zip', 'r') as myzip:
    myzip.extractall()
#import tarzip
#with tarzip.
#+END_SRC
已经掌握了数据整理的第一步：收集数据。在这个数据集中，这意味着：
- 从互联网下载文件，在这种情况下文件是来自 Kaggle 的 zip 文件，
- 打开 Jupyter Notebook，
- 使用 Python 解压压缩文件，
- 然后将解压的 CSV 文件导入 Jupyter Notebook 的 pandas DataFrame。
*** 评估数据assess（用pandas查看info,等）
低质量数据通常被称为脏数据，脏数据存在内容问题。不整洁数据通常被称为 "杂乱" 数据，
杂乱数据存在结构问题。

最好将所有评估记录在数据整理模板评估部分的底部，即清洗标题的正上方。定义清洗操作时，
参考这些记录可使数据清洗更简单，还可以避免使你手忙脚乱。
- +修正+ 意义不明的非描述性header(记录问题时应只用名词，问题修改后再用动诩表示已经完成)
- ~df.info()~ ~df.head()~ ~df.tail()~ ~df.value_counts()~

编程数据清洗过程：
- 定义
- 编码
- 测试

定义指以书面形式定义数据清洗计划，其中我们需将评估转变为定义的清洗任务。
这个计划也可作为一个指导清单，所以其他人（或我们自己将来）也可以回顾和重现自己的工作。

编码指将这些定义转换为代码并执行该代码。

测试指测试我们的数据集，通常使用代码，以确保有效完成我们的清洗工作。
*** 清洗
- header中不要有点号 ~.~ ，不然就不能用 ~df.age~ 来索引。
#+BEGIN_SRC python :results output
df_calen = df_clean.rename(columns={'oldname1': 'newname1', 'oldname2': 'newname2'})
(assert 'ASAP' not in x for x in df_clean.columns) # 可以这样写吗？
#+END_SRC
*** 重新评估与迭代
整个整理数据的流程通常是要反复迭代的，即使在完成分析之后。
*** 整理、EDA与ETL
** lesson2：收集数据
#+BEGIN_SRC python :results output
import pandas as pd
pd.read_csv('file.csv', sep='\t')
#+END_SRC
*** 从网页(HTML文件)中抓取数据
- 将HTML文件保存在本地（如用Request库），并将文件读入 ~BeautifulSoup~ 构造函数中
- 将HTML响应内容直接读入 ~BeautifulSoup~ 构造函数（如用Request库）
#+BEGIN_SRC python :results output
import requests
url = 'https://www.rottentomatoes.com/m/et_the_extraterrestrial'
response = requests.get(url)
# save html to file
# work with html memory
#+END_SRC
[[https://www.udacity.com/course/intro-to-html-and-css--ud001][关于HTML和CSS的介绍课程]]
*** html简介
在两个 ~<body>~ 标签之间的内容需要重点关注。如：
#+BEGIN_SRC html
<body>
<p>This is a paragraph</p>
<h1>this is heading1</h1>
<span>this is a span</span>
<h2>this is heading1</h2>
<h3>this is heading1</h3>
<body>
#+END_SRC
都被前后标签包含，后标签有slash。
*** 树结构
被包含就是子结构，如 ~h1~ ~p~ 是 ~div~ 的子结构。
#+BEGIN_SRC html
<body>
<div>
<h1>this is heading1</h1>
<p>This is a paragraph</p>
<h2>this is heading1</h2>
</div>
<body>
#+END_SRC
一个有用的小技巧：
#+BEGIN_SRC python :results output
s = 'this is a random string to test a little trick.'
print(s[:-len('trick.')]) # 去掉末尾的字符串
#+END_SRC

#+RESULTS:
: this is a random string to test a little
*** BeautifulSoup（HTML解析器）
BeautifulSoup是用Python语言写的HTML解析器（不用再自己动手解析了，
如写正则表达式寻找字符串）。
#+BEGIN_SRC python :results output
from bs4 import BeautifulSoup
with open('rt_html/et.html') as file:
    soup = BeautifulSoup(file, "lxml")
soup.find('title').contents[0][:-len('tomato')]
#+END_SRC

练习:

根据对 HTML 文件结构的了解，你将使用 Beautiful Soup 来提取对于每个 HTML 文件，
我们所需的观众评分指标和观众评分得数，以及上面视频中的电影标题（所以我们稍后将合并数据集），
然后将它们保存在 pandas DataFrame 中。你的任务是提取每个 HTML 文件的标题（电影名）、
观众评分和参与评分观众人数，并三个一组作为字典附加到 df_list 。

#+BEGIN_SRC python :results output
from bs4 import BeautifulSoup
import os
folder = 'rt_html'
df_list = []
for html_file in os.listdir(folder):
    with open(os.path.join(folder, html_file), 'r') as f:
        soup = BeautifulSoup(f, 'lxml')
        title = soup.find('title').contents[0][:-len('tomato')]
        score = soup.find('div', class_='audience-score meter').find('span').contents[0][:-1]
        rating_counts = soup.find('div', class_='audience-info')# 方法类似，可以逐层打印，寻找标签位置
        rating_counts = rating_counts.find_all('div')[1].contents[1].strip().replace(',', '')
        d = {'title': title,
             'audience_score': int(score),
             'number_audience_ratings': int(rating_counts)}
        df_list.append(d)
df = pd.DataFrame(df_list, columns=['title', 'audience_score', 'number_audience_ratings'])
#+END_SRC
还需要合并两个DataFrame
*** 用编程的方式下载网络上的文件（http协议+Python Requests）
HTTP全称为超文本传输协议，是web浏览器和Web服务器之间的沟通语言。
#+BEGIN_SRC python :results output
import requests
import os
folder_name = 'ebert_reviews'
if not os.path.exists(folder_name):
    os.makedirs(folder_name)
url = 'https://classroom.udacity.com/nanodegrees/nd002-cn-advanced-vip/parts/4ec06ac9-9e53-42c2-a53d-3b4ec9d7e25e/modules/fea8de18-62f3-4b23-9f19-4293ee51871f/lessons/96402d84-c99d-4982-9edf-2430ef30d222/concepts/ed908f34-ce67-44c0-acb1-d81abd5d9e37'
response = requests.get(url)
with open(os.path.join(folder_name, 'down_html.txt'), 'wb') as f:
    f.write(response.content)
print(response) # 200代表请求成功

#+END_SRC

#+RESULTS:
: <Response [200]>
*** 关于编码和字符集
[[https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/][请查看这两篇文章]]
*** Python中的文本文件
打开目录下的文件可以这样：
- ~import os;for file in os.listdir(folder_name):~ ，
  这样会打开所有目录下的文件。
- 如果想打开此目录下的特定文件可以用通配符，
  ~import glob;for file in glob.glob('/ebert-reviews/*.tst')~ 。

去掉末尾的换行符可以用 ~line[:-1]~
#+BEGIN_SRC python :results output
with open(file_name, 'r', encoding='utf-8') as f:
    title = f.readline()[:-1] # 去掉末尾的换行符
    txt = f.read() # 将剩余内容读入
    d = {'title': title,
         'txt': txt}
    df_list.append(d)
df = pd.DataFrame(df_list) # 将网页内容存在DataFrame中
#+END_SRC
*** 用API（应用程序编程接口）或访问库来下载网页
有的API可以下载图片，但有的不可以。特定网站的API由网站自己提供，如烂蕃茄的rtsimple,
维基百科的MediaWiki.[[https://www.mediawiki.org/wiki/API:Main_page#A_simple_example][这是MeidaWiki的使用tutorial]]。

[[https://www.mediawiki.org/wiki/API:Client_code#Python][这是MediaWiki的python访问库列表]]
*** JSON文件结构
大多数API文件板式都是JSON，它被用来贮存相对复杂的文件内容。JSON代表javascript object notation

JSON文件结构是字典的形式，而且key必须是字符串，值可以是字符串、list、数字等。它也可以嵌套。
JSON 有六种有效的数据类型，其中两种可使层次数据在采用大部分格式时具有灵活性：
- JSON 数组（被Python解释为list）
- JSON 对象（被Python解释为dictionary）
这些在 Python 中有类似的数据结构，所以可以使用相同的方法访问。
*** 18.混搭：API、以编程方式下载文件和 JSON
至此
*** 收集：总结
收集数据是数据整理过程的第一步：
- 收集
- 评估
- 清理
根据数据来源及其格式，收集数据的步骤也不同。

高级收集过程：
- 获取数据(从互联网下载文件、抓取网页、查询 API 等)
- 将数据导入编程环境(例如 Jupyter Notebook)
** TODO lesson3: 维基百科爬虫
   - State "TODO"       from              [2020-04-09 Thu 12:23]
** TODO lesson4: 抓取豆瓣电影信息（案例演练）
   - State "TODO"       from              [2020-04-09 Thu 12:23]
** lesson5: 评估数据 assess
*** intro
在清理之前要评估，不评估就不知道数据的问题在哪，怎么去清理。
数据问题：
- 数据质量问题（缺失、重复、错误等）（脏）
- 数据整洁度问题（结构问题）（乱）
解决方法：
- 目测寻找
- 编程寻找（info()等, 可视化EDA）
检测问题、记录问题，以便再现。建议在数据整理过程中，将评估和清理步骤分开进行。所以，第一步仅填写观察值是个不错的做法。

但是如果你在评估之后，马上就对数据进行处理/清理/解决，这也是中很好的方法。
如果是这样的话，你就可以略过观察的步骤，直接进行清理（这是 Define-Code-Test
 清理框架的一部分，我们将在第 4 课介绍）。

目测也是了解数据集的一个步骤，要评估，你要先理解这个行列代表的意思及这个数据集的目标及背景知识。
*** ~数据质量问题~ 的几个度量指标
- 完整性（有无NAN？）
- 有效性（如负的身高等）
- 准确性（如身高1cm）
- 一致性（格式相同）
*** 编程式的评估 ~数据质量问题~
#+BEGIN_SRC python :results output
df['age'].duplicated() # 某列重复的数据，返回boolen数组
df['age'].value_counts() # 与上有类似的功能，返回值出现的次数
df['age'].sort_values(ascending=False) # 数值型值的排序
#+END_SRC
- 要留意同一客体的不同称谓产生多条记录的问题，可以用某些（不太可能重复但）重复的属性来检查。
- 要留意object类型的列，是不是有数据类型不一致问题？（比如有空值 ~-~ ，但没有被pandas识别）
*** 总结
按以上大纲助逐排查是的重要的！而且收集、评估、清理、分析过程在任何时候都是可迭代的，即你可以随机
收集、评估、清理、分析。
** lesson6: 清理数据
*** 大纲
数据清理流程：确定方案，编写代码，检验效果
- 先解决数据缺失问题（一般要先解决完整性问题，为什么？）
- 再解决整洁度问题
- 最后解决质量问题

人工vs程序清理？除非只需要一次，否则不要人工清理，应该用程序清理。
先备份数据，不要在原始的脏乱数据上操作！备份用 ~df.copy()~
#+BEGIN_SRC python :results output
import pandas as pd
df = pd.read_csv('file.csv')
df_clean = df.copy()
df_clean['animal'] = df_clean['animal'].str[2:]
df_clean['animal'] = df_clean['animal'].str.replace('!', '')
#+END_SRC
*** 先处理缺失值
[[https://goo.gl/3bgcc8][Imputation教程]]
#+BEGIN_SRC python :results output
import pandas as pd
# 用正则表达式提取字符串
df['email'] = df.contact.str.extract('([a-zA-Z][a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+[a-zA-Z])', expand=True)
df['phone'] = df.contact.str.extract('((?:\+?\d{1,2}\s)?\(?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4})', expand=True)
# 拼接DataFrame
df_new = df1.append(df_2) # 列对齐
# 行对齐
# melt融合DataFrame
# split劈裂DataFrame
# merge合并DataFrame，按照名和姓进行
df = pd.merge(df, df2, on=['given_name', 'surname'], how='left')
#+END_SRC
[[https://regexone.com/][正则表达式教程]]
[[https://pandas.pydata.org/pandas-docs/stable/generated/pandas.melt.html][融合函数]]
[[https://blog.csdn.net/maymay_/article/details/80039677][融合函数2]]
这一节课需要更多的学习和练习，对pandas清理的常用函数还不熟悉，经验也不多！
** TODO 项目：清洗与分析数据
   - State "TODO"       from              [2020-04-09 Thu 12:23]
* Python数据可视化
主要有两个方面：
- 探索性数据可视化
用来寻找变量间的关系或隐藏的见解，不必完美，只为自己看。
- 解释性数据可视化
呈现给观众，为了用图或表来回答之前提出的问题。

收集数据、清理数据、探索数据、分析数据（建立模型）、呈现分析结果
** lesson1: 数据分析中的数据可视化
*** 可视化的重要性
有时数据集的一些度量指标完全相同，但实际上却完全不同，如它斯库姆四重奏。
*** 可视化库
Matplotlib（复杂而灵活）, Seaborn（简单而高效）, pandas（简单而高效）.
应该在灵活性与高效性之间权衡。可以先用pandas和Seaborn，来探索，最后用Matplotlib来
呈现。
*** 本课程大纲
- 可视化设计
- 数据探索
- 解释性可视化
- 可视化案例分析
** lesson2: 可视化的设计
*** 数据类型
数据的四个测量级别：
- 分类类型（非数值类型、定性类型） ~柱状图~ ~饼状图~
  - 无序分类变量（Nominal data）
  - 有序分类变量 （Ordinal data）
- 数值类型（Numeric、定量类型） ~直方图~ ~图~
  - 定距变量（Interval data）：绝对差有意义，可以进行加减运算
  - 定比变量（Ratio data）：相对差有意义，可以进行乘除运算
  或者也可以分为：
  - 离散变量
  - 连续变量
在探索数据的时候，首先要考虑的事情就是判断数据是分类的还是数值的。

图表垃圾指的是图表中对理解要展示的信息并无帮助，或者使读者无法关注到重要信息的所有视觉元素。

提高数据墨水比ink-data-ratio
*** 使用颜色
- 首选黑白（黑，白，灰）
- 如果要用颜色，尽量不要用鲜艳的颜色（如天然色或淡色）
- 颜色的选择要有助于信息的表达，凸显你的信息并区分兴趣组，避免因为要有颜色而添加颜色
- 针对色盲人群：不要用红绿区分数据，用蓝橙代替
*** 额外的视觉编码
颜色与形状是分类变量最好的展现方式，而标志大小有助于数值型数据的表达。只有在绝对必要时才使用这些额外的编码。
如果一个图表中有太多的信息，建议将这些信息分解为多个单独的信息，这样听众反而可以更好地理解信息的各个方面。
** lesson3: 单变量数据探索
数据探索应该从单变量探索开始，这有助于理解数据集。
*** 条形图（柱状图）
- 研究分类类型变量的分布，首选用bar chart, 基线应设为0。
- 对于无序的分类类型，可以按出现频次从大到小排序
- 但不要对有序的分类类型排序，因为它的本身的顺序更重要
- 可以选择，要不要使用横向的bar chart
- 可以选择是用绝对次数还是出现的相对频率来绘图
#+BEGIN_SRC python :results output
import seaborn as sb
import pandas
import matplotlib.pyplot as plt
%matplotlib inline
type_order = df.type1.value_counts().index
sb.countplot(data=df, x='type1', color=sb.color_palette()[0], order=type_order)
# 横向的bar chart 只需要将改成y
sb.countplot(data=df, y='type1', color=sb.color_palette()[0], order=type_order)
# 可以使用 matplotlib 的 xticks 函数及其 "rotation" 参数更改绘制刻度标记的方向
plt.xtics(rotation=90)
#+END_SRC
*** 缺失值统计
用 ~sb.barplot~ 来绘图。
#+BEGIN_SRC python :results output
na_counts = df.isna().sum()
base_color = sb.color_palette()[0]
sb.barplot(na_counts.index.values, na_counts, color = base_color)
#+END_SRC
*** 饼状图
饼图是一种应用场合很有限的图表类型，图表创建者很容易将饼图绘制得难以看懂。如果你要使用饼图，请尝试遵守下面的规则：

- 确保你关心的是相对频率。每个扇区应该表示整体的一部分，而不是单独的数值（除非变量能够求和成某个整体）。
- 将扇区限制在一定数量内。饼图最好只包含两到三个扇区，如果扇区足以明确区分，也可以包含四到五个。如果你有很多个类别，
  并且某些类别所占的比例很小，那可以将它们组合到一起，或者将这些比例很小的类别放到 "其他" 类别中。
- 系统地绘制数据。绘制饼图的一种常见方法是从圆圈的顶部开始，然后沿着顺时针方向绘制每个分类级别，从最常见的到最不常见的排列。
  如果有三个类别，并且想要对比其中两个，一种常见绘制方法是将这两个类别放在 12 点钟方向的两侧，第三个类别填充在底部剩余部分。

如果无法满足这些规则，则建议使用条形图。通常选择条形图更保险。长条高度比面积或角度更精确，并且条形图比饼图更紧凑。
对于值很多的变量来说，条形图更灵活。

参考：[[https://classroom.udacity.com/nanodegrees/nd002-cn-advanced-vip/parts/3114eb46-e9e5-422f-938f-288b78fa4ccd/modules/1dc09d28-5703-493c-aab5-a418b8bfa3e1/lessons/b86503df-e416-4f0e-9e2d-a7a3c08d0bc3/concepts/e7d15a6d-4d4b-418e-b85e-3cf0ed6f6740][udacity的饼图]]
*** 直方图
不会为每个单独的数值绘制一个长条，而是定义几个连续的分组（bin），为每个分组绘制长条以代表相应的数字。
x表示特征值，y表示数量统计（与柱状图相同），绘图时应尝试不同的组距。 ~plt.hist(data=df, x='age', bins=30)~
** lesson4: 双变量可视化探索（研究相关性）
- 数值变量vs数值变量 用 scatterplots（散点图）
- 数值变量vs分类变量 用 violin plots（小提琴图）
- 分类变量vs分类变量 用 clustered bar charts（分组柱状图）
*** 重叠、透明度和抖动
如果要绘制大量数据点，或者数值变量是离散型的，那么直接使用散点图可能无法呈现足够的信息。图形可能会出现重叠，由于大量数据重叠到一起，
导致很难看清变量之间的关系。在这种情形下，我们需要应用透明度和抖动，使散点图能呈现更多的信息。

除了设置透明度，我们还可以通过抖动使每个点稍微偏离真实值所对应的位置。这并不是 scatter 函数中的直接选项，但是 seaborn 的regplot
函数有这个内置选项。可以单独添加 x 轴和 y 轴抖动，不会影响到回归方程的拟合情况
#+BEGIN_SRC python :results output
sb.regplot(data = df, x = 'disc_var1', y = 'disc_var2', fit_reg = False,
           x_jitter = 0.2, y_jitter = 0.2, scatter_kws = {'alpha' : 1/3})
#+END_SRC
*** 热图
*** violinplot
Seaborn 的 violinplot 函数可以创建将小提琴图和箱线图相结合的图表
#+BEGIN_SRC python :results output
sb.violinplot(data = df, x = 'cat_var', y = 'num_var')
#+END_SRC


* project3: 探索性数据分析EDA
** 什么是EDA
EDA可以增加你对数据的了解，对提出假设和建立模型有帮助。进行EDA时要时刻对数据保持好奇与怀疑。
在EDA时应该让数据直接说话，并测试自己对数据集的直觉，培养新的直觉。

协调迁移

交互式可视化用d3.js
** TODO R基础
   - State "TODO"       from              [2020-04-09 Thu 13:24]
* 用数据来呈述你的结论
* project4: 求职-模拟面试
* 机器学习机器
* 监督学习
* 非监督学习
* project6: 深度学习
* 软件工程
* 数据工程
* project7: 推荐系统与试验设计
* project8: Spark与大数据
* github个人资料实战
