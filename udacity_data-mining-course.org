#+author: ligy
#+email: li.gaoyang@foxmail.com
#+date: <2020-04-07 Tue>

udacity的核心课程：数据挖掘工程师直通班，纳米学位。目标：在3到7天的时间里，完成所有数据挖掘课程，
同时还要进行项目练习。

总结：project1和project2的许多内容有所重复，project1以概括为主。project2更加深入，
但也不是很深入。虽然project2的许多函数我都用过，并且也可能使用地更细致，但这种课程形式
的讲解正是我所欠缺的。课程的学习可以将之前学会的零碎知识串联起来，对big picture有更好地
理解。可以消除只见树木不见森林的迷惘，与不踏实感，是很有必要的。主体结构有了之后，再自学就
更高效了。
* project1: 用python进行基础数据分析
** 数据分析的五个步骤
- 提问
- 整理数据wrangling（收集，评估，清理）
- 执行EDA
- 得出结论（或甚至是做出预测，通常使用机器学习和推理性统计完成），本课使用描述性统计
- 传达结果

第 1 步：提问
你要么获取一批数据，然后根据它提问，要么先提问，然后根据问题收集数据。在这两种情况下，好的问题可以帮助你将精力集中在数据的相关部分，并帮助你得出有洞察力的分析。

第 2 步：整理数据
你通过三步来获得所需的数据：收集，评估，清理。你收集所需的数据来回答你的问题，评估你的数据来识别数据质量或结构中的任何问题，并通过修改、替换或删除数据来清理数据，以确保你的数据集具有最高质量和尽可能结构化。

第 3 步：执行 EDA（探索性数据分析）
你可以探索并扩充数据，以最大限度地发挥你的数据分析、可视化和模型构建的潜力。探索数据涉及在数据中查找模式，可视化数据中的关系，并对你正在使用的数据建立直觉。经过探索后，你可以删除异常值，并从数据中创建更好的特征，这称为特征工程。

第 4 步：得出结论（或甚至是做出预测）
这一步通常使用机器学习或推理性统计来完成，不在本课程范围内，本课的重点是使用描述性统计得出结论。

第 5 步：传达结果
你通常需要证明你发现的见解及传达意义。或者，如果你的最终目标是构建系统，则通常需要分享构建的结果，解释你得出设计结论的方式，并报告该系统的性能。传达结果的方法有多种：报告、幻灯片、博客帖子、电子邮件、演示文稿，甚至对话。数据可视化总会给你呈现很大的价值。
*** 收集数据、评估数据
csv为comma separated values

#+BEGIN_SRC python :results output
import pandas as pd
df = pd.read_csv('file', header=1) # 使用第二行为标题，删除上面行的内容
df = pd.read_csv('student_scores.csv', header=None) # 没标题
labels = ['id', 'name', 'attendance', 'hw', 'test1',
'project1', 'test2', 'project2', 'final'] # 自定义标题
df = pd.read_csv('student_scores.csv', names=labels)
# 替换数据的标题行
df = pd.read_csv('student_scores.csv', header=0, names=labels)
#将一个或多个列指定为数据框的索引
df = pd.read_csv('student_scores.csv', index_col='Name')
#+END_SRC

#+RESULTS:
*** 清理数据
常见问题：类型错误、数据缺失、数据冗余（重复）、结构问题等。
#+BEGIN_SRC python :results output
# 用平均值填充
df['col_miss'].fillna(df['col_miss'].mean(), inplace=True)
# 查找数据重复情况
df.duplicated()
# 大数据集用
sum(df.duplicated())
# 去重复，查看更多参数
df.drop_duplicates(inplace=True)
# 将日期转成datatime类型。每次载入数据都需要转换！
df['dates'] = pd.to_datetime(df['dates'])
# 字符串是否被包含?
print('ac' in 'acbs')
# 为数据框中的列分配新标签
df.columns = new_labels
# 保存以后用
df.to_csv('new_file.csv', index=False)
#+END_SRC

*** 执行EDA（探索性数据分析）
DataFrame和Series对matplotlib有封装的绘图函数，但比较简单，复杂的还要用matplotlib.
#+BEGIN_SRC python :results output
# 在notebook中查看绘图
% matplotlib inline
# 用分号来隐藏不需要的输出
# hist绘直方图
df.hist(figsize=(10, 8));
# 也可以用在Series上
df['age'].hist();
# 统计值的出现次数，并绘图
df['age'].value_counts().plot(kind='bar') # 直方图
df['age'].value_counts().plot(kind='pie') # 饼状图
df.plot(x='age', y='height', kind='scatter')
df['age'].plot(kind='box'); #箱图
#+END_SRC

*** 得出结论
*** 传达结果
** Python包概述
包是函数和类等的集合，数据分析最常用的有Numpy, pandas, matplotlib
#+BEGIN_SRC python :results output
import numpy as np
l = list(range(9))
print(np.mean(l))
#+END_SRC

#+RESULTS:
: 4.0
* TODO project1的项目: Google Play Store Analysis
  - State "TODO"       from              [2020-04-09 Thu 12:23]
[[https://www.jianshu.com/p/04d180d90a3f][pandas入门]]

[[https://tonysyu.github.io/raw_content/matplotlib-style-gallery/gallery.html][图表呈现风格]]
* project2:数据整理
** lesson1：数据整理简介
最好用程序下载数据（API），而不是在网页直接下载。
*** 用python解压缩文件
zipfile是个上下文管理器，支持with语句。
#+BEGIN_SRC python :results output
import zipfile
with zipfile.ZipFile('file.zip', 'r') as myzip:
    myzip.extractall()
#import tarzip
#with tarzip.
#+END_SRC
已经掌握了数据整理的第一步：收集数据。在这个数据集中，这意味着：
- 从互联网下载文件，在这种情况下文件是来自 Kaggle 的 zip 文件，
- 打开 Jupyter Notebook，
- 使用 Python 解压压缩文件，
- 然后将解压的 CSV 文件导入 Jupyter Notebook 的 pandas DataFrame。
*** 评估数据assess（用pandas查看info,等）
低质量数据通常被称为脏数据，脏数据存在内容问题。不整洁数据通常被称为 "杂乱" 数据，
杂乱数据存在结构问题。

最好将所有评估记录在数据整理模板评估部分的底部，即清洗标题的正上方。定义清洗操作时，
参考这些记录可使数据清洗更简单，还可以避免使你手忙脚乱。
- +修正+ 意义不明的非描述性header(记录问题时应只用名词，问题修改后再用动诩表示已经完成)
- ~df.info()~ ~df.head()~ ~df.tail()~ ~df.value_counts()~

编程数据清洗过程：
- 定义
- 编码
- 测试

定义指以书面形式定义数据清洗计划，其中我们需将评估转变为定义的清洗任务。
这个计划也可作为一个指导清单，所以其他人（或我们自己将来）也可以回顾和重现自己的工作。

编码指将这些定义转换为代码并执行该代码。

测试指测试我们的数据集，通常使用代码，以确保有效完成我们的清洗工作。
*** 清洗
- header中不要有点号 ~.~ ，不然就不能用 ~df.age~ 来索引。
#+BEGIN_SRC python :results output
df_calen = df_clean.rename(columns={'oldname1': 'newname1', 'oldname2': 'newname2'})
(assert 'ASAP' not in x for x in df_clean.columns) # 可以这样写吗？
#+END_SRC
*** 重新评估与迭代
整个整理数据的流程通常是要反复迭代的，即使在完成分析之后。
*** 整理、EDA与ETL
** lesson2：收集数据
#+BEGIN_SRC python :results output
import pandas as pd
pd.read_csv('file.csv', sep='\t')
#+END_SRC
*** 从网页(HTML文件)中抓取数据
- 将HTML文件保存在本地（如用Request库），并将文件读入 ~BeautifulSoup~ 构造函数中
- 将HTML响应内容直接读入 ~BeautifulSoup~ 构造函数（如用Request库）
#+BEGIN_SRC python :results output
import requests
url = 'https://www.rottentomatoes.com/m/et_the_extraterrestrial'
response = requests.get(url)
# save html to file
# work with html memory
#+END_SRC
[[https://www.udacity.com/course/intro-to-html-and-css--ud001][关于HTML和CSS的介绍课程]]
*** html简介
在两个 ~<body>~ 标签之间的内容需要重点关注。如：
#+BEGIN_SRC html
<body>
<p>This is a paragraph</p>
<h1>this is heading1</h1>
<span>this is a span</span>
<h2>this is heading1</h2>
<h3>this is heading1</h3>
<body>
#+END_SRC
都被前后标签包含，后标签有slash。
*** 树结构
被包含就是子结构，如 ~h1~ ~p~ 是 ~div~ 的子结构。
#+BEGIN_SRC html
<body>
<div>
<h1>this is heading1</h1>
<p>This is a paragraph</p>
<h2>this is heading1</h2>
</div>
<body>
#+END_SRC
一个有用的小技巧：
#+BEGIN_SRC python :results output
s = 'this is a random string to test a little trick.'
print(s[:-len('trick.')]) # 去掉末尾的字符串
#+END_SRC

#+RESULTS:
: this is a random string to test a little
*** BeautifulSoup（HTML解析器）
BeautifulSoup是用Python语言写的HTML解析器（不用再自己动手解析了，
如写正则表达式寻找字符串）。
#+BEGIN_SRC python :results output
from bs4 import BeautifulSoup
with open('rt_html/et.html') as file:
    soup = BeautifulSoup(file, "lxml")
soup.find('title').contents[0][:-len('tomato')]
#+END_SRC

练习:

根据对 HTML 文件结构的了解，你将使用 Beautiful Soup 来提取对于每个 HTML 文件，
我们所需的观众评分指标和观众评分得数，以及上面视频中的电影标题（所以我们稍后将合并数据集），
然后将它们保存在 pandas DataFrame 中。你的任务是提取每个 HTML 文件的标题（电影名）、
观众评分和参与评分观众人数，并三个一组作为字典附加到 df_list 。

#+BEGIN_SRC python :results output
from bs4 import BeautifulSoup
import os
folder = 'rt_html'
df_list = []
for html_file in os.listdir(folder):
    with open(os.path.join(folder, html_file), 'r') as f:
        soup = BeautifulSoup(f, 'lxml')
        title = soup.find('title').contents[0][:-len('tomato')]
        score = soup.find('div', class_='audience-score meter').find('span').contents[0][:-1]
        rating_counts = soup.find('div', class_='audience-info')# 方法类似，可以逐层打印，寻找标签位置
        rating_counts = rating_counts.find_all('div')[1].contents[1].strip().replace(',', '')
        d = {'title': title,
             'audience_score': int(score),
             'number_audience_ratings': int(rating_counts)}
        df_list.append(d)
df = pd.DataFrame(df_list, columns=['title', 'audience_score', 'number_audience_ratings'])
#+END_SRC
还需要合并两个DataFrame
*** 用编程的方式下载网络上的文件（http协议+Python Requests）
HTTP全称为超文本传输协议，是web浏览器和Web服务器之间的沟通语言。
#+BEGIN_SRC python :results output
import requests
import os
folder_name = 'ebert_reviews'
if not os.path.exists(folder_name):
    os.makedirs(folder_name)
url = 'https://classroom.udacity.com/nanodegrees/nd002-cn-advanced-vip/parts/4ec06ac9-9e53-42c2-a53d-3b4ec9d7e25e/modules/fea8de18-62f3-4b23-9f19-4293ee51871f/lessons/96402d84-c99d-4982-9edf-2430ef30d222/concepts/ed908f34-ce67-44c0-acb1-d81abd5d9e37'
response = requests.get(url)
with open(os.path.join(folder_name, 'down_html.txt'), 'wb') as f:
    f.write(response.content)
print(response) # 200代表请求成功

#+END_SRC

#+RESULTS:
: <Response [200]>
*** 关于编码和字符集
[[https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/][请查看这两篇文章]]
*** Python中的文本文件
打开目录下的文件可以这样：
- ~import os;for file in os.listdir(folder_name):~ ，
  这样会打开所有目录下的文件。
- 如果想打开此目录下的特定文件可以用通配符，
  ~import glob;for file in glob.glob('/ebert-reviews/*.tst')~ 。

去掉末尾的换行符可以用 ~line[:-1]~
#+BEGIN_SRC python :results output
with open(file_name, 'r', encoding='utf-8') as f:
    title = f.readline()[:-1] # 去掉末尾的换行符
    txt = f.read() # 将剩余内容读入
    d = {'title': title,
         'txt': txt}
    df_list.append(d)
df = pd.DataFrame(df_list) # 将网页内容存在DataFrame中
#+END_SRC
*** 用API（应用程序编程接口）或访问库来下载网页
有的API可以下载图片，但有的不可以。特定网站的API由网站自己提供，如烂蕃茄的rtsimple,
维基百科的MediaWiki.[[https://www.mediawiki.org/wiki/API:Main_page#A_simple_example][这是MeidaWiki的使用tutorial]]。

[[https://www.mediawiki.org/wiki/API:Client_code#Python][这是MediaWiki的python访问库列表]]
*** JSON文件结构
大多数API文件板式都是JSON，它被用来贮存相对复杂的文件内容。JSON代表javascript object notation

JSON文件结构是字典的形式，而且key必须是字符串，值可以是字符串、list、数字等。它也可以嵌套。
JSON 有六种有效的数据类型，其中两种可使层次数据在采用大部分格式时具有灵活性：
- JSON 数组（被Python解释为list）
- JSON 对象（被Python解释为dictionary）
这些在 Python 中有类似的数据结构，所以可以使用相同的方法访问。
*** 18.混搭：API、以编程方式下载文件和 JSON
至此
*** 收集：总结
收集数据是数据整理过程的第一步：
- 收集
- 评估
- 清理
根据数据来源及其格式，收集数据的步骤也不同。

高级收集过程：
- 获取数据(从互联网下载文件、抓取网页、查询 API 等)
- 将数据导入编程环境(例如 Jupyter Notebook)
** TODO lesson3: 维基百科爬虫
   - State "TODO"       from              [2020-04-09 Thu 12:23]
** TODO lesson4: 抓取豆瓣电影信息（案例演练）
   - State "TODO"       from              [2020-04-09 Thu 12:23]
** lesson5: 评估数据 assess
*** intro
在清理之前要评估，不评估就不知道数据的问题在哪，怎么去清理。
数据问题：
- 数据质量问题（缺失、重复、错误等）（脏）
- 数据整洁度问题（结构问题）（乱）
解决方法：
- 目测寻找
- 编程寻找（info()等, 可视化EDA）
检测问题、记录问题，以便再现。建议在数据整理过程中，将评估和清理步骤分开进行。所以，第一步仅填写观察值是个不错的做法。

但是如果你在评估之后，马上就对数据进行处理/清理/解决，这也是中很好的方法。
如果是这样的话，你就可以略过观察的步骤，直接进行清理（这是 Define-Code-Test
 清理框架的一部分，我们将在第 4 课介绍）。

目测也是了解数据集的一个步骤，要评估，你要先理解这个行列代表的意思及这个数据集的目标及背景知识。
*** ~数据质量问题~ 的几个度量指标
- 完整性（有无NAN？）
- 有效性（如负的身高等）
- 准确性（如身高1cm）
- 一致性（格式相同）
*** 编程式的评估 ~数据质量问题~
#+BEGIN_SRC python :results output
df['age'].duplicated() # 某列重复的数据，返回boolen数组
df['age'].value_counts() # 与上有类似的功能，返回值出现的次数
df['age'].sort_values(ascending=False) # 数值型值的排序
#+END_SRC
- 要留意同一客体的不同称谓产生多条记录的问题，可以用某些（不太可能重复但）重复的属性来检查。
- 要留意object类型的列，是不是有数据类型不一致问题？（比如有空值 ~-~ ，但没有被pandas识别）
*** 总结
按以上大纲助逐排查是的重要的！而且收集、评估、清理、分析过程在任何时候都是可迭代的，即你可以随机
收集、评估、清理、分析。
** lesson6: 清理数据
*** 大纲
数据清理流程：确定方案，编写代码，检验效果
- 先解决数据缺失问题（一般要先解决完整性问题，为什么？）
- 再解决整洁度问题
- 最后解决质量问题

人工vs程序清理？除非只需要一次，否则不要人工清理，应该用程序清理。
先备份数据，不要在原始的脏乱数据上操作！备份用 ~df.copy()~
#+BEGIN_SRC python :results output
import pandas as pd
df = pd.read_csv('file.csv')
df_clean = df.copy()
df_clean['animal'] = df_clean['animal'].str[2:]
df_clean['animal'] = df_clean['animal'].str.replace('!', '')
#+END_SRC
*** 先处理缺失值
[[https://goo.gl/3bgcc8][Imputation教程]]
#+BEGIN_SRC python :results output
import pandas as pd
# 用正则表达式提取字符串
df['email'] = df.contact.str.extract('([a-zA-Z][a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+[a-zA-Z])', expand=True)
df['phone'] = df.contact.str.extract('((?:\+?\d{1,2}\s)?\(?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4})', expand=True)
# 拼接DataFrame
df_new = df1.append(df_2) # 列对齐
# 行对齐
# melt融合DataFrame
# split劈裂DataFrame
# merge合并DataFrame，按照名和姓进行
df = pd.merge(df, df2, on=['given_name', 'surname'], how='left')
#+END_SRC
[[https://regexone.com/][正则表达式教程]]
[[https://pandas.pydata.org/pandas-docs/stable/generated/pandas.melt.html][融合函数]]
[[https://blog.csdn.net/maymay_/article/details/80039677][融合函数2]]
这一节课需要更多的学习和练习，对pandas清理的常用函数还不熟悉，经验也不多！
** TODO 项目：清洗与分析数据
   - State "TODO"       from              [2020-04-09 Thu 12:23]
* Python数据可视化
主要有两个方面：
- 探索性数据可视化
用来寻找变量间的关系或隐藏的见解，不必完美，只为自己看。
- 解释性数据可视化
呈现给观众，为了用图或表来回答之前提出的问题。

收集数据、清理数据、探索数据、分析数据（建立模型）、呈现分析结果
** lesson1: 数据分析中的数据可视化
*** 可视化的重要性
有时数据集的一些度量指标完全相同，但实际上却完全不同，如它斯库姆四重奏。
*** 可视化库
Matplotlib（复杂而灵活）, Seaborn（简单而高效）, pandas（简单而高效）.
应该在灵活性与高效性之间权衡。可以先用pandas和Seaborn，来探索，最后用Matplotlib来
呈现。
*** 本课程大纲
- 可视化设计
- 数据探索
- 解释性可视化
- 可视化案例分析
** lesson2: 可视化的设计
*** 数据类型
数据的四个测量级别：
- 分类类型（非数值类型、定性类型） ~柱状图~ ~饼状图~
  - 无序分类变量（Nominal data）
  - 有序分类变量 （Ordinal data）
- 数值类型（Numeric、定量类型） ~直方图~ ~图~
  - 定距变量（Interval data）：绝对差有意义，可以进行加减运算
  - 定比变量（Ratio data）：相对差有意义，可以进行乘除运算
  或者也可以分为：
  - 离散变量
  - 连续变量
在探索数据的时候，首先要考虑的事情就是判断数据是分类的还是数值的。

图表垃圾指的是图表中对理解要展示的信息并无帮助，或者使读者无法关注到重要信息的所有视觉元素。

提高数据墨水比ink-data-ratio
*** 使用颜色
- 首选黑白（黑，白，灰）
- 如果要用颜色，尽量不要用鲜艳的颜色（如天然色或淡色）
- 颜色的选择要有助于信息的表达，凸显你的信息并区分兴趣组，避免因为要有颜色而添加颜色
- 针对色盲人群：不要用红绿区分数据，用蓝橙代替
*** 额外的视觉编码
颜色与形状是分类变量最好的展现方式，而标志大小有助于数值型数据的表达。只有在绝对必要时才使用这些额外的编码。
如果一个图表中有太多的信息，建议将这些信息分解为多个单独的信息，这样听众反而可以更好地理解信息的各个方面。
** lesson3: 单变量数据探索
数据探索应该从单变量探索开始，这有助于理解数据集。
*** 条形图（柱状图）
- 研究分类类型变量的分布，首选用bar chart, 基线应设为0。
- 对于无序的分类类型，可以按出现频次从大到小排序
- 但不要对有序的分类类型排序，因为它的本身的顺序更重要
- 可以选择，要不要使用横向的bar chart
- 可以选择是用绝对次数还是出现的相对频率来绘图
#+BEGIN_SRC python :results output
import seaborn as sb
import pandas
import matplotlib.pyplot as plt
%matplotlib inline
type_order = df.type1.value_counts().index
sb.countplot(data=df, x='type1', color=sb.color_palette()[0], order=type_order)
# 横向的bar chart 只需要将改成y
sb.countplot(data=df, y='type1', color=sb.color_palette()[0], order=type_order)
# 可以使用 matplotlib 的 xticks 函数及其 "rotation" 参数更改绘制刻度标记的方向
plt.xtics(rotation=90)
#+END_SRC
*** 缺失值统计
用 ~sb.barplot~ 来绘图。
#+BEGIN_SRC python :results output
na_counts = df.isna().sum()
base_color = sb.color_palette()[0]
sb.barplot(na_counts.index.values, na_counts, color = base_color)
#+END_SRC
*** 饼状图
饼图是一种应用场合很有限的图表类型，图表创建者很容易将饼图绘制得难以看懂。如果你要使用饼图，请尝试遵守下面的规则：

- 确保你关心的是相对频率。每个扇区应该表示整体的一部分，而不是单独的数值（除非变量能够求和成某个整体）。
- 将扇区限制在一定数量内。饼图最好只包含两到三个扇区，如果扇区足以明确区分，也可以包含四到五个。如果你有很多个类别，
  并且某些类别所占的比例很小，那可以将它们组合到一起，或者将这些比例很小的类别放到 "其他" 类别中。
- 系统地绘制数据。绘制饼图的一种常见方法是从圆圈的顶部开始，然后沿着顺时针方向绘制每个分类级别，从最常见的到最不常见的排列。
  如果有三个类别，并且想要对比其中两个，一种常见绘制方法是将这两个类别放在 12 点钟方向的两侧，第三个类别填充在底部剩余部分。

如果无法满足这些规则，则建议使用条形图。通常选择条形图更保险。长条高度比面积或角度更精确，并且条形图比饼图更紧凑。
对于值很多的变量来说，条形图更灵活。

参考：[[https://classroom.udacity.com/nanodegrees/nd002-cn-advanced-vip/parts/3114eb46-e9e5-422f-938f-288b78fa4ccd/modules/1dc09d28-5703-493c-aab5-a418b8bfa3e1/lessons/b86503df-e416-4f0e-9e2d-a7a3c08d0bc3/concepts/e7d15a6d-4d4b-418e-b85e-3cf0ed6f6740][udacity的饼图]]
*** 直方图
不会为每个单独的数值绘制一个长条，而是定义几个连续的分组（bin），为每个分组绘制长条以代表相应的数字。
x表示特征值，y表示数量统计（与柱状图相同），绘图时应尝试不同的组距。 ~plt.hist(data=df, x='age', bins=30)~
** lesson4: 双变量可视化探索（研究相关性）
- 数值变量vs数值变量 用 scatterplots（散点图）
- 数值变量vs分类变量 用 violin plots（小提琴图）
- 分类变量vs分类变量 用 clustered bar charts（分组柱状图）
*** 重叠、透明度和抖动
如果要绘制大量数据点，或者数值变量是离散型的，那么直接使用散点图可能无法呈现足够的信息。图形可能会出现重叠，由于大量数据重叠到一起，
导致很难看清变量之间的关系。在这种情形下，我们需要应用透明度和抖动，使散点图能呈现更多的信息。

除了设置透明度，我们还可以通过抖动使每个点稍微偏离真实值所对应的位置。这并不是 scatter 函数中的直接选项，但是 seaborn 的regplot
函数有这个内置选项。可以单独添加 x 轴和 y 轴抖动，不会影响到回归方程的拟合情况
#+BEGIN_SRC python :results output
sb.regplot(data = df, x = 'disc_var1', y = 'disc_var2', fit_reg = False,
           x_jitter = 0.2, y_jitter = 0.2, scatter_kws = {'alpha' : 1/3})
#+END_SRC
*** 热图
*** violinplot
Seaborn 的 violinplot 函数可以创建将小提琴图和箱线图相结合的图表
#+BEGIN_SRC python :results output
sb.violinplot(data = df, x = 'cat_var', y = 'num_var')
#+END_SRC
** lesson5: 多变量可视化探索
** lesson6: 解释性数据可视化
*** 回顾数据分析的过程
让我们简要回顾一下数据分析过程，看一下解释性可视化和探索性可视化在数据分析的各个过程中的适用性。 数据分析的五个主要步骤为 ：

+ 提炼 - 从电子表格、SQL、网络等途径获得数据
+ 清洗 - 在这一步，可能会用到探索性可视化
+ 探索 - 这一步使用探索性可视化
+ 分析 - 在这一步中，探索性和解释性可视化都有可能用到
+ 分享 - 这里就是解释性可视化的用武之地
前面三章的内容主要在讲探索性数据分析。在探索过程中创建的图表主要是給数据分析师自己看的，所以不会被特别地修饰或者美化，
能够从数据中获得见解即可 。

而本章课程主要讲如何根据你的数据洞察和见解，继而创作解释性数据分析可视化。此类可视化侧重于讲述你想要传达的特定故事。
很多时候，在这个过程中创建的可视化是由探索过程创建的图表演化而来，我们 对这些探索性可视化进行额外的修饰，以突出想要展示的特定关键信息。你的图表不光要包含丰富的信息，而且要具有吸引力和可解释性，所以先让我们来回顾之前课上提到的可视化设计的概念。
*** 用数据讲故事
包括以下步骤：
- 先抛出问题
- 重复是个好事情
- 突出问题的答案
- 呼吁读者采取行动
*** 修饰图表
到目前为止，你学到的代码都只是能让你创建图表，够用就行，没有考虑美观因素。但为了使你的发现准确和有效地传达给受众，你需要学会修饰图表。
在修饰图表的时候，有很多需要考虑的问题。

- 选择合适的图表类型
  图表类型的选择取决于你的变量数量以及它们的类型，比如它们是有序分类，还是无序分类，是连续数值，还是离散数值。图表类型的选择还取决于你想要传达的变量之间的关系。比如，选择小提琴图，箱线图还是调整过的条形图，取决于你有多少数据以及数据分布是不是你关心的重点。如果你有很多数据而且它们的分布是有意义的，你很可能会选择小提琴图；但如果如果你的数据量不多而且数据的分布并没有那么重要，那你会更倾向于使用箱线图或条形图。
- 选择合适的编码
  变量不光会影响你选择的图表类型，而且也影响你选择的编码类型。比如说，如果你有三个数值变量，不能随机将变量用在 x 轴、y 轴或用颜色编码。一般情况下，放在坐标轴上的变量应该相对重要，如果有一个变量是因变量或者结果变量，那么你应该把它放在 y 轴上。在其他情况下，因变量也可以用颜色来编码，就像从上俯视由其他两个自变量组成的平面一样。
- 注意整体考量与诚实设计原则
  在设置图表参数的时候，请务必记住之前课上提到的可视化设计原则。
- 你应该尽可能地确保图表中没有很多图表垃圾并且拥有比较高的数据墨水比。应该在必须的情况下，即你想传达额外信息的情况下，再决定加入非位置编码。比如，在单独的频率条形图上使用颜色可能没必要，但如果在其他图表中也使用相同的颜色表示相应的变量，那么使用颜色也是合理的。同理，你应当避免对不同的变量使用相同的颜色，减少读者的困惑。
- 最后，还要遵守诚实设计原则，避免对数据做出扭曲或者不真实的可视化呈现。如果你使用条形图或直方图，那 y 轴最好从 0 开始。如果你使用了任何坐标轴变换，最好在你的标题，坐标轴标签以及刻度标记上进行说明。

轴标签以及选择合适的轴刻度
坐标轴一定要包含相应的标签。在探索性分析的时候，这可能没那么地重要，因为图表主要是給你自己看的，而且代码也都是你自己写的。但是当你要将图表包含的信息传达给别人的时候，这变得非常得关键。当你添加坐标轴标签的时候，也尽可能提供轴变量的单位。

至于轴刻度，你应该在每个轴上提供至少三个刻度标记 。这对于已经变换的数据尤为重要，为了清晰地展示数据的比例，你可能需要应用足够多的刻度标记。如果数字非常大或非常小，你应该考虑使用缩写（比如，用 ”250k” 取代 “250000”）。

为非位置编码的变量提供图例
一定要为那些非坐标轴变量提供图例。对于颜色编码，你可以在图表旁边加上颜色栏。需要特别注意的是，就像你会添加坐标轴标签，也请为你的图例提供一些解释性的标签。

为图表提供标题和描述性文本
最后，记住要为你的图表提供具有描述性的标题。如果这是个包含了重要信息的关键图表，尽可能把重要的信息放在标题里以吸引观众的注意力，而不是简单地把图表中的变量当作标题。

虽然图表是我们传达信息的主要工具之一，但是我们也可以有一些其他辅助工具，比如在图表下方或者周围加入一些描述性文本注释，强调重点，这样能使你的观众更容易地获得重要的信息或者增强记忆。

- 用 Matplotlib 来修饰图表
  在之前单变量可视化的课程中，你了解了 Matplotlib 和 Seaborn 是如何绘图的：每个图表都是一个单独的 Figure 对象，这个对象包含了坐标轴，坐标轴又包含了用来表达数据的点，线或长条等。理解并利用这种结构会让你的图表修饰工作变得容易。下面的每个函数都包含其文档页面的链接， 以及与之关联的对象类型。

figure (Figure)： 用来创建新的图表。 可以用它来初始化图表，最常用的是 “figsize” 参数设置图表大小。
xlabel 和 ylabel (Axes)： 用来设置轴标签。
xticks 和 yticks (Axes)： 用来设置轴刻度。
legend (Axes)： 用来创建和自定义图例。一个关键参数是 "title"，可以为你的图例提供描述文字，标记特征名称。"loc" 和 "ncol" 参数可以改变图例的位置和形状，因为有些时候默认的图例位置可能并不理想。
colorbar (Axes)： 用来添加调色板。用 "label" 这个参数給调色板添加标签。
title (Axes)： 用来设置单个坐标系图表的标题。
suptitle (Figure)： 用来设置整个图表容器的标题。suptitle 和 title 主要的不同是前者是为整个图表容器（Figure 对象） 设置标题，而后者只是为单个坐标系的图表（Axes 对象）设置标题。这对于分面图表或者创建包含很多子图的图表很有用，suptitle 可以为整个图表 矩阵设置总标题。
所有上面提到的函数以及在 Seaborn 里和这些函数相关的参数，在整个课程中用到的比较少。下面是用到这些函数的几个示例。

这个例子用到了 汽车燃油效率数据集。因为图表用到了颜色栏，所以用了 figsize 来将图表 变大一些。标题，坐标轴以及颜色栏也都包含了标签。注意每个标签里都将变量的单位标注在了括号里面。
#+BEGIN_SRC python :results output
# loading in the data, sampling to reduce points plotted
fuel_econ = pd.read_csv('./data/fuel_econ.csv')

np.random.seed(2018)
sample = np.random.choice(fuel_econ.shape[0], 200, replace = False)
fuel_econ_subset = fuel_econ.loc[sample]

# plotting the data
plt.figure(figsize = [7,4])
plt.scatter(data = fuel_econ_subset, x = 'displ', y = 'comb', c = 'co2',
            cmap = 'viridis_r')
plt.title('Fuel Efficiency and CO2 Output by Engine Size')
plt.xlabel('Displacement (l)')
plt.ylabel('Combined Fuel Eff. (mpg)')
plt.colorbar(label = 'CO2 (g/mi)');
#+END_SRC

* project3: 探索性数据分析EDA
** 什么是EDA
EDA可以增加你对数据的了解，对提出假设和建立模型有帮助。进行EDA时要时刻对数据保持好奇与怀疑。
在EDA时应该让数据直接说话，并测试自己对数据集的直觉，培养新的直觉。

协调迁移

交互式可视化用d3.js
** TODO R基础
   - State "TODO"       from              [2020-04-09 Thu 13:24]
* 用数据来呈述你的结论
* project4: 求职-模拟面试
* 机器学习机器
* 监督学习
* 非监督学习
* project6: 深度学习
* 软件工程
* 数据工程
* project7: 推荐系统与试验设计
* project8: Spark与大数据
* github个人资料实战
