# -*- org -*-
#+TITLE: 《机器学习实战》笔记
#+AUTHOR: GoldenRaven
#+DATE: <2020-02-27 Thu>
#+EMAIL: li.gaoyang@foxmail.com
# #+OPTIONS: num:t

#+BEGIN_COMMENT
#+BEGIN_SRC sh :session
bash crop-convert.bash
#+END_SRC

#+RESULTS:
| /home/ligy/Documents/MachineLearning_notebook/pdfs |       |            |         |           |          |           |    |       |           |
| PDFCROP                                            | 1.38, | 2012/11/02 | -       | Copyright | (c)      | 2002-2012 | by | Heiko | Oberdiek. |
| ==>                                                |     1 | page       | written | on        | `1.pdf'. |           |    |       |           |
| softmax.pdf                                        |       |            |         |           |          |           |    |       |           |

#+END_COMMENT
#+ATTR_HTML: :width 300
[[file:images/handson.jpg]]

这是我在学习Aurelien Geron的书籍《机器学习实战》时自己的总结，欢迎留言。
* 注意
- 对收入分层抽样，不能分太多层
- 分层方法：除以1.5，向上取整；然后合并大于5的分类
- 地理数据可视化，用其他相关属性作为颜色，和散点大小
- 寻找与标签相关性高的属性，用 ~df.corr()['labels']~
- 进一步考察高相关性属性的数据模式，并删除可能的错误数据
- 尝试不同的属性组合，以找到高相关性特征
- 将预测器与标签分离，因为可能不一定对它们使用相同的转换方式
- 特征缩放（归一化、标准化），即同比缩放所有属性
- 评估训练得的模型，对训练集求RMSE或MAE
- 误差较大则拟合不足，可以
- 误差过小？则用验证集来验证得到的模型，以检查是否过拟合
- 交叉验证，可以sklearn的K-fold功能
- 如果在验证集上得到的误差大则说明确实有过拟合，需要更换模型
- 尝试多个模型以找到2-5个有效的模型，别花太多时间去调整超参数
- 保存每个尝试过的模型，用pickel或sklearn的joblib
- 训练集分数明显低于验证集分数，则过度拟合
- 注意：目标值一般不进行绽放，并且只对训练集缩放
* 第四章 训练（线性）模型
** 4.1 (纯)线性回归 Linear Regression
用以描述线性化数据集，模型或假设（hypothesis）是特征（x）的线性函数,或者写成向量形式，令x_0 = 1:
#+ATTR_HTML: :width 400
[[file:images/linear_hypothsis.png]]

上面的表达式也称之为回归方程（regression equation），\theta为回归系数。
成本函数，MSE函数：
#+attr_html: :width 400px
[[file:images/MSE.png]]

*** 4.1.1 闭式解-标准方程（normal equation）
即直接通过解析表达式计算得到参数向量\theta:
#+attr_html: :width 200px
[[file:images/normal_equation.png]]

可以使用Numpy的线性代数模块np.linalg中的inv()函数来求矩阵逆，用dot()方法计算内积。
特征数量大时标准方程计算极其缓慢，此时可以用迭代优化法。
#+BEGIN_EXAMPLE
注意：书中有误，Scikit-Learn的LinearRegression类并不是标准方程的实现，而是基于X的SVD分解。其时间复杂度为O(n^2)，在m<n或特征线性相关时依然可以工作（标准方程不行，因为不满秩）。
LinearRegression类不需要对特征进行标度。
#+END_EXAMPLE
*** 4.1.2 闭式解-X矩阵的SVD分解
#+BEGIN_SRC python
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression() # 基于scipy.linalg.lstsq()
lin_reg.fit(X, y)
lin_reg.intercept_, lin_reg.coef_ # 偏置\theta_0与权重\theta_i
lin_reg.predict(X_new) # 预测
#+END_SRC
也可以直接调用lstsq()，意为最小平方：

#+BEGIN_SRC python
theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)
theta_best_svd
#+END_SRC
*** 4.1.3 梯度下降(迭代优化)
从随机值开始，每一步降低成本函数，直到成本函数最小值。每一步的步长取决于超参数: /学习率/ /\eta/ ( /learning rate/ ).
# #+BEGIN_EXAMPLE
注意：
1. 线性回归模型的MSE是凸函数，没有局部最小，只一个全局最小。
2. 应用梯度下降时要保证所有特征数值大小比例差不多，即要先进行特征缩放！
3. 特征缩放主要有两种方式：standerization和normalization，见第二章，68页。
4. 可以使用sklearn的StandardScaler类。
5. 学习率的选取很关键，可以限制迭代次数进行网格搜索。
# #+END_EXAMPLE
**** 4.1.2.1 批量梯度下降
在计算梯度下降的每一步时，都基于整个训练集。训练集庞大时很耗时，但随特征数增大时，算法表现良好。
**** 4.1.2.2 随机梯度下降
在计算梯度下降的每一步时，只随机地使用一个训练集实例。训练集庞大时很耗时，但随特征数增大时，算法表现良好。
- 当成本函数有局部最小时，可以跳出局部最小，找到全局最小
- 设定 /学习计划/ ，开始时大步长，最后小步长（模拟退火）
- 乱序训练集使一个接一个地使用实例，反而会导致收敛更慢！
#+BEGIN_SRC python
from sklearn.linear_model import SGDRegressor
sgd_reg = SGDRegressor(max_iter=50, tol=-np.infty, penalty=None, eta0=0.1, random_state=42)
sgd_reg.fit(X, y.ravel())
sgd_reg.intercept_, sgd_reg.coef_
#+END_SRC
**** 4.1.2.3 小批量梯度下降
在计算梯度下降的每一步时，只随机地使用一个小的实例集。主要优势在于可以用GPU加速计算。
*** 4.1.4 标准方程与梯度下降对比
|--------------------------+----------------------------------------|
| 梯度下降（Gradient descending） | 标准方程（Normal equation）              |
|--------------------------+----------------------------------------|
| 需要选择适当的学习率\eta | 不需要学习率\eta                       |
|--------------------------+----------------------------------------|
| 需要多次迭代             | 直接解析求解                           |
|--------------------------+----------------------------------------|
| 在特征很多时仍工作很好   | 复杂度O(n^3)，特征矩阵维度大时不宜考虑   |
|--------------------------+----------------------------------------|
| 能应用在更复杂的算法中（如逻辑回归） | 需要矩阵可逆（满秩）                   |
|--------------------------+----------------------------------------|
** 4.2 多项式回归 Polynomial Regression
也称为多元线性回归，所以也属于线性回归，即使用以拟合非线性数据集。从参数\theta的角度看，这个模型将线性回归特征的高次幂项作为新的特征，并将它们线性组合起来，所以依然属于线性模型。
*** 4.2.1 训练集增广
将原特征的次幂项作为新的特征加入训练集，在这个拓展过的特征集上训练线性模型。可以使用sklearn的PolynomialFeatures类来进行：
#+BEGIN_SRC python
from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)
lin_reg = LinearRegression()
lin_reg.fit(X_poly, y)
lin_reg.intercept_, lin_reg.coef_
#+END_SRC

#+BEGIN_EXAMPLE
注意：
1. 高次幂项也包括特征的交叉项
2. 作用PolynomialFeatures类要小心特征数量爆炸！
#+END_EXAMPLE
*** 4.2.2 学习曲线
在使用模型时要经常判断：模型是否过度拟合或者拟合不足？
- 一种是第二章中学习的，使用交叉验证来评估模型的泛化性能。如果在训练集上表现比交叉验证的泛化表现好很多，则是过度拟合。如果两者表现都不佳，则拟合不足。
- 还有一种，即观察学习曲线。
曲线绘制的是模型在训练集和验证集上，关于训练集大小的性能函数。要绘制这个函数，要在不同大小的训练集上多次训练模型。

*判断标准* ：
- 拟合不足：两线均到达高地，十分接近，且相当高。
- 过度拟合：训练集误差远小于一般标准，且两条线之间有一定差距。

*改进方法* :
- 拟合不足：增加模型复杂程度
- 过度拟合：提供更多数据，或约束模型（正则化）
*** 4.2.3 偏差/方差权衡
增加模型复杂度会显著减少模型的偏差，增加拟合的方差;相反，降低模型复杂度会显著提升模型的偏差，降低拟合的方差。
** 4.3 正则线性模型（线性模型的正则化）
对多项式模型来说，正则化的简单方法是降低多项式除数;对线性模型来说，正则化通常通过约束模型的权重来实现，比如有如下三种不同的实现方法：岭回归、套索回归、弹性网络。
*** 4.3.1 岭回归 Ridge Regression
也叫吉洪诺夫正则化，在成本函数中添加一个正则项 \alpha/2 \sum_{i=1}^{n} \theta_{i}^{2}。
#+BEGIN_EXAMPLE
注意：正则化只能在训练时添加到成本函数，完成训练后要用未经正则化的性能指标来评估模型性能。
#+END_EXAMPLE
岭回归的成本函数：
#+BEGIN_CENTER
J(\theta) = MSE(\theta) + \alpha/2*\sum_{i=1}^{n} \theta_{i}^{2}
#+END_CENTER
超参数\alpha 控制正则化程度，\alpha=0时回复到线性模型，\alpha 非常大时所有权重都接近于零，结果是一条穿过数据平均值的水平线。正则项是权重向量 *\theta* 的l_{2}范数平方的一半。
#+BEGIN_EXAMPLE
注意：
1. 求和从i=1开始，对偏置项不正则化。
2. 执行岭回归前，要对数据进行缩放（大多数正则化模型都需要）。
#+END_EXAMPLE
与线性回归相同，可以直接闭式解，也可以使用随机梯度下降。sklearn的Ridge执行闭式解法，
利用Andre-Louis Cholesdy的矩阵因式分解：
#+BEGIN_SRC python
from sklearn.linear_model import Ridge
# ridge_reg = Ridge(alpha=1, solver="sag", random_state=42)
ridge_reg = Ridge(alpha=1, solver="cholesky", random_state=42)
ridge_reg.fit(X, y)
ridge_reg.predict([[1.5]])
#+END_SRC
使用随机梯度下降的代码如下：
#+BEGIN_SRC python
sgd_reg = SGDRegressor(max_iter=50, tol=-np.infty, penalty="l2", random_state=42)
sgd_reg.fit(X, y.ravel())
sgd_reg.predict([[1.5]])
#+END_SRC
其中的penalty参数为惩罚的类型。
*** 4.3.2 套索回归 Lasso Regression
套索回归是另一种正则化方法，也叫最小绝对收缩和选择算子回归（Least Absolute Shrinkage and Selection Operator Regression），简称Lasso。它为成本函数增加的一项是权重向量的l_{1}范数。Lasso回归的成本函数为：
#+BEGIN_CENTER
J(\theta) = MSE(\theta) + \alpha \sum_{i=1}^{n} |\theta_{i}|
#+END_CENTER
Lasso回归倾向于完全消除最不重要特征的权重，换句话说，它会自动执行特征选择并输出一个稀疏模型（即只有少量特征的权重非零）。sklearn的Lasso类 +实现的是什么算法？+
#+BEGIN_SRC python
from sklearn.linear_model import Lasso
lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(X, y)
lasso_reg.predict([[1.5]])
#+END_SRC
与岭回归一样，也可以使用随机梯度下降，代码如下：
#+BEGIN_SRC python
sgd_reg = SGDRegressor(max_iter=50, tol=-np.infty, penalty="l1", random_state=42)
sgd_reg.fit(X, y.ravel())
sgd_reg.predict([[1.5]])
#+END_SRC
*** 4.3.3 弹性网络 Elastic Net
弹性网络是岭回归和Lasso回归的中间地带，其正则项是它们正则项的混合，比例由r来控制。r=0时相当于岭回归，r=1时相当于Lasso回归。其成本函数为：
#+BEGIN_CENTER
J(\theta) = MSE(\theta) + r\alpha \sum_{i=1}^{n} |\theta_{i}| + (1-r)\alpha/2*\sum_{i=1}^{n} \theta_{i}^{2}
#+END_CENTER
sklearn的ElasticNet类代码如下：
#+BEGIN_SRC python
from sklearn.linear_model import ElasticNet
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)
elastic_net.fit(X, y)
elastic_net.predict([[1.5]])
#+END_SRC
同样可以用随机梯度下降来实现弹性网络正则化，如下：
#+BEGIN_SRC python
sgd_reg = SGDRegressor(max_iter=50, tol=-np.infty, penalty="elasticnet", random_state=42)
elastic_net.fit(X, y)
elastic_net.predict([[1.5]])
#+END_SRC
*** 4.3.4 如何在线性回归和以上三种回归之中选择呢？
通常而言，有正则化总比没有强，所以大多数时候应该避免使用纯线性回归。岭回归是个不错的默认选择，但如果你觉得实际用到的特征只有少数几个，那就应该更倾向于Lasso或弹性网络，因为它们可以对特征进行自动选择。一般而言，弹性网络优于Lasso回归，因为当特征数大于训练实例数或特征强相关时，Lasso回归可能非常不稳定。
*** 4.4.4 早期停止法
对于梯度下降等迭代算法，还有一个正则化方法，就是在验证误差达到最小误差时停止训练。（可以先观察是否真正达到最小误差）
#+BEGIN_SRC python
from sklearn.base import clone
sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True, penalty=None,
                       learning_rate="constant", eta0=0.0005, random_state=42)

minimum_val_error = float("inf")
best_epoch = None
best_model = None
for epoch in range(1000):
    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off
    y_val_predict = sgd_reg.predict(X_val_poly_scaled)
    val_error = mean_squared_error(y_val, y_val_predict)
    if val_error < minimum_val_error:
        minimum_val_error = val_error
        best_epoch = epoch
        best_model = clone(sgd_reg)
#+END_SRC
** 4.4 逻辑回归 Logistic Regression
一些回归算法也被用于分类任务，反之亦然。逻辑回归依然是线性模型。
逻辑回归，也叫罗吉思回归，被广泛用于估算一个实例属于某个特定类别的概率。如果预概率测超过50%，则判定为正类，反之则为负类。这样它就成一个二元分类器。
与线性回归不同的是，它用 *\theta^{T}\cdot X* 的sigmoid函数值作为概率值，而不是 *\theta^{T}\cdot X* 本身：

#+attr_html: :width 500px
[[file:images/logistic.png]]

#+BEGIN_COMMENT
#+BEGIN_SRC latex :file pdfs/logistic.pdf
\[
\hat{p} = h_\theta(\textbf{X}) = \sigma(\theta^T\cdot \textbf{X})
\]
#+END_SRC

#+RESULTS:
#+BEGIN_LaTeX
[[file:pdfs/logistic.pdf]]
#+END_LaTeX
#+END_COMMENT
\sigma(t)是sigmoid函数：

#+BEGIN_COMMENT
#+BEGIN_SRC latex :file pdfs/sigmoid.pdf
\[
\sigma(t) = \frac{1}{(1+exp(-t))}
\]
#+END_SRC

#+RESULTS:
#+BEGIN_LaTeX
[[file:pdfs/sigmoid.pdf]]
#+END_LaTeX
#+END_COMMENT

#+attr_html: :width 500px
[[file:images/sigmoid.png]]

成本函数为log损失函数：

#+BEGIN_COMMENT
#+BEGIN_SRC latex :file pdfs/cost_log.pdf
\[
J(\theta) = - \frac{1}{m}\sum_{i=1}^{n} [y^{(i)}\rm{log}(
\hat{p}^{(i)}) + (1-y^{(i)})\rm{log}(1-\hat{p}^{(i)})]
\]
#+END_SRC

#+RESULTS:
#+BEGIN_LaTeX
[[file:pdfs/cost_log.pdf]]
#+END_LaTeX
#+END_COMMENT

#+attr_html: :width 500px
[[file:images/cost_log.png]]

这个函数没有闭式解，只能迭代优化，而且它是个凸函数。可以用随机梯度下降等优化算法求解。
如下：

#+begin_src python
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(solver="liblinear", random_state=42)
log_reg.fit(X, y)
#+end_src

*** 决策边界
决策边界，顾名思义，就是用来划清界限的边界，边界的形态可以不定，可以是点，可以是线，
也可以是平面。Andrew Ng 在公开课中强调：“决策边界是预测函数h_{\theta}(x)的属性，
而不是训练集属性”，这是因为能作出“划清”类间界限的只有h_{\theta}(x)，而训练集只是用来
训练和调节参数的。

决策边界由h_{\theta}(x) = \theta^{T} \cdot X = 0定义，所以如果h_{\theta}(x)
函数是线性的，那么决策边界就是线性的;如果h_{\theta}(x)是非线性的，那么决策边界就是非
线性的。

#+begin_example
注意： 与上述多项式回归同理，虽然决策边界是非线性的，但是模型依然是线性的。
#+end_example

*** 逻辑回归的正则化
与其他线性模型一样，逻辑回归也可以用“l_{1}”, “l_{2}”或“elasticnet”惩罚函数来正则化，
默认是l_{2}函数。sklearn的LogisticRegression类中控制正则化程度的超参为C，
是\alpha 的逆反，（其他线性模型为\alpha ），C越 +大+ 小，正则化程度越大。
** 4.5 多元逻辑回归 Softmax Regression
对于多分类问题，如前所述，可以采用OvA策略，也可采用OvO策略。OvA指为每个类别分别训练一
个二分类器，用以识别是否是该类别，对于特定实例取最近的类别为预测类别。即将多分类转化成
多次二分类问题。OvO策略指任何两个类别训练一个二分类器，如MNIST中，要训练C_{10}^{2}=45
个二分类器。识别时对一个实例运行C_{10}^{2}个二分类器，最后以获胜次数多的类别作用预测
结果。OvO的优点在于，训练时只需要对部分训练数据进行（只需要在需要区分的两个类别的训练集上
进行）。

#+BEGIN_EXAMPLE
注意：只有对于在大数据集上表现糟糕的算法（SVM），OvO是优先的选择;对于大多数二元分类器来说，OvA策略更好。
#+END_EXAMPLE

Softmax回归是逻辑回归的推广，可以直接支持多类别，不需要训练并组合多个二元分类器。
对于一个特定实例 *x*, Softmax 回归会计算出每个类别k的分数s_{k}(*x*), 然后应用
softmax函数（也叫归一化指数），估算每个类别的概率。softmax分数：

#+BEGIN_COMMENT
#+BEGIN_SRC latex :file pdfs/softmax.pdf
\[
s_{k}(\textbf{x}) = \theta_{k}^T \cdot \textbf{x}
\]
#+END_SRC

#+RESULTS:
#+BEGIN_LaTeX
[[file:pdfs/softmax.pdf]]
#+END_LaTeX
#+END_COMMENT

#+attr_html: :width 500px
[[file:images/softmax.png]]

每个类别都有自己的权重向量 *\theta_{k}*, 所有这些向量通常作为行，存贮在参数矩阵
\Theta 中。

有了类别分数后，实例 *x* 属于类别k的概率被定义为：

#+BEGIN_COMMENT
#+BEGIN_SRC latex :file pdfs/softmax2.pdf
\[
\hat{p}_{k} = \sigma(\textbf{s}(\textbf{x}))_{k} = \frac{exp(s_{k}(\textbf{x}))}{\sum_{j=1}^{K}exp(s_{j}(\textbf{x}))}
\]
#+END_SRC

#+RESULTS:
#+BEGIN_LaTeX
[[file:pdfs/softmax2.pdf]]
#+END_LaTeX
#+END_COMMENT

#+attr_html: :width 500px
[[file:images/softmax2.png]]

预测类别\hat{y} 是概率\hat{p}_k 最大的类别k：

#+BEGIN_COMMENT
#+BEGIN_SRC latex :file pdfs/softmax-pred.pdf
\[
\hat{y} = argmax_{k} \sigma(\textbf{s}(\textbf{x})) = argmax_{k} (\theta_{k}^T \cdot \textbf{x})
\]
#+END_SRC

#+RESULTS:
#+BEGIN_LaTeX
[[file:pdfs/softmax-pred.pdf]]
#+END_LaTeX
#+END_COMMENT

#+attr_html: :width 500px
[[file:images/softmax-pred.png]]

#+BEGIN_EXAMPLE
注意： Softmax回归器每次只能预测一个类别，也就是说它是多类别，但不是多输出。所以仅适用于互斥的类别。
#+END_EXAMPLE

我们已经知道了模型怎么估算概率，并做出预测，那怎么训练呢？需要最小化的成本函数（交叉熵）：


#+BEGIN_COMMENT
#+BEGIN_SRC latex :file pdfs/cross-entropy.pdf
\[
J(\Theta) = -\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} y_{k}^{(i)} log(\hat{p}_{k}^{(i)})
\]
#+END_SRC

#+RESULTS:
#+BEGIN_LaTeX
[[file:pdfs/cross-entropy.pdf]]
#+END_LaTeX
#+END_COMMENT

#+attr_html: :width 500px
[[file:images/cross-entropy.png]]

它来源于信息理论，描述的是多类别预测的准确性。两个离散概率分布p和q之间的交叉熵定义为：

#+BEGIN_COMMENT
#+BEGIN_SRC latex :file pdfs/cross2.pdf
\[
H(p, q) = \sum_{x} p(x) \rm{log}\ q(x)
\]
#+END_SRC

#+RESULTS:
#+BEGIN_LaTeX
[[file:pdfs/cross2.pdf]]
#+END_LaTeX
#+END_COMMENT

#+attr_html: :width 500px
[[file:images/cross2.png]]

对于这个成本函数可以作用随机梯度下降或其他优化算法找到最优解参数矩阵\Theta （每个类别
的权重向量 *\theta_{k}* ）。在sklearn中，当对多个类别进行训练时
LogisticRegression会默认选择OvA策略。将参数multi_class设置为"multinomial"
可以将其切换成Softmax回归。还要指定一个支持Softmax回归的求解器。默认使用l_{2}正则化，
用超参C控制。代码如下：

#+BEGIN_SRC python
softmax_reg = LogisticRegression(multi_class="multinomial",solver="lbfgs", C=10, random_state=42)
softmax_reg.fit(X, y)
y_proba = softmax_reg.predict_proba(X_new) # 预测特定实例的每个类别概率
y_predict = softmax_reg.predict(X_new) # 预测特定实例的类别
#+END_SRC

- 此时的决策边界如何理解？

#+attr_html: :width 600px
[[file:images/decision_boundary.png]]
* 第五章 支持向量机（SVM, support vector machine）
#+BEGIN_QUOTE
只读一本书不是学习，而是娱乐。
#+END_QUOTE
支持向量机可以用于 ~线性~ 、 ~非线性~ 的 ~分类~ 和 ~回归~ 任务，也可以用
于 ~异常值检测~ 任务。SVM是机器学习领域最受欢迎的模型之一，特别适用于中小型复杂
数据集的分类。

** 线性SVM分类 :硬间隔分类:软间隔分类:
数据集可以分为线性可分和线性不可分，也就是说可不可以用一条直线轻松地分开。
#+begin_example
疑问：线性SVM分类器是个二分类器？
#+end_example

~支持向量机分类器~ 的 ~基本思想~ ：拟合类别之间可能的、最宽“街道”，因此也
叫大间隔分类。决策边界完全由位于街道上的（包括街道边缘上的）实例所决定（支
持），这些实例称为支持向量。预测结果时只涉及支持向量，而不涉及整个训练集。

#+begin_example
注意：SVM对特征的缩放特别敏感，如果不缩放，SVM将趋于忽略小的特征。
#+end_example

- 硬间隔分类：严格要求所有实例都不在街道上，且都位于正确的一侧。存在问题：
  + 1. 数据集一定是线性可分时才有效。
  + 2. 对异常值十分敏感。
- 软间隔分类：尽可能地保持街道宽阔、同时限制间隔违例（位于街道之上、甚至在错误一
边的实例）

上述过程可以看作是在正则化，在sklearn的 ~SVM~ 类中，超参数C控制这个平衡：
C越小，街道越宽，间隔违例越多，正则化越强;反则反之。在sklearn中可以有以下实现：
- 使用 ~LinearSVC~ 类， ~LinearSVC(C=1, loss='hinge')~ 。它会对偏置项作正则化，
所以要先减去平均值使训练集集中。可以使用StandardScaler进行。超参数loss='hinge'，
dual='False'。快速收敛。
- 也可选择SVC类， ~SVC(kernel='linear', C=1)~ ，但比LinearSVC慢得多，
尤其对于大型数据集而言。不推荐使用。
- 还可以使用 ~SGDClassifier(loss='hinge', alpha=1/(m*C))~,
这适于常规随机梯度下降来训练SVM分类器。并不快速收敛，但对大数据集或在线分类任务有效。

#+begin_example
注意：
  1. 如果你的SVM模型过拟合了，试试减小C来进行正则化
  2. 与Logistic回归分类器不同，SVM不会输出类别概率，可以设probability=True，
     来用Logistic回归对SVM校准，可以获得predict_proba()和predict_log_proba()方法
#+end_example
sklearn的LinearSVC示例代码如下：

#+BEGIN_SRC python
import numpy as np
from sklearn import datasets
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC

iris = datasets.load_iris()
X = iris["data"][:, (2, 3)]  # petal length, petal width
y = (iris["target"] == 2).astype(np.float64)  # Iris-Virginica

svm_clf = Pipeline([
        ("scaler", StandardScaler()),
        ("linear_svc", LinearSVC(C=1, loss="hinge", random_state=42)),
        #("SVC_liear-kenel", SVC(kernel='linear', C=1)),
        #("SGDClassifier", SGDClassifier(loss='hinge', alpha=1/(C*m))),
    ])

svm_clf.fit(X, y)
svm_clf.predict([[5.5, 1.7]])
#+END_SRC
** 非线性SVM分类
通常情况下线性SVM分类器是有效的，而且出人意料地好。但有些数据集是线性不可分的。
处理的方法之一是添加更多特征（数据集增广），比如多项式特征。
*** 添加多项式特征（数据集增广）
在某些情况下，这可以使数据集线性可分。实现起来非常简单，而且对所有机器学习算法都很有效。

#+BEGIN_EXAMPLE
思路：对原数据集添加（比如多项式）特征，使得数据集线性可分，再使用线性SVM分类器解决。
#+END_EXAMPLE

用sklearn的实现如下：

#+BEGIN_SRC python
from sklearn.datasets import make_moons # 插入卫星数据集
from sklearn.pipeline import Pipeline # 包含管道
from sklearn.preprocessing import PolynomialFeatures # 引入多项式特征转换器

polynomial_svm_clf = Pipeline([
        ("poly_features", PolynomialFeatures(degree=3)), # 添加三阶多项式特征
        ("scaler", StandardScaler()), # 缩放特征
        ("svm_clf", LinearSVC(C=10, loss="hinge", random_state=42)) # 调用线性SVM分类
    ])

polynomial_svm_clf.fit(X, y) # 训练数据
#+END_SRC

*** 多项式核
添加多项式特征虽然有效，但存在这样的问题：
  1. 如果多项式阶数太低，不能处理复杂数据集
  2. 如果阶数太高，会创造出大量特征（特征爆炸），导致模型太慢

Fortunately, 有一个数学技巧可以不用真正地添加特征，但其效果就同添加了一样，这个技巧叫
~核技巧~.这里多次提到的核技巧会在后面说到。

sklearn的SVC类实现了核技巧，如表格[[sklearn线性SVM分类算法对比]]所示。代码如下：

#+BEGIN_SRC python
from sklearn.svm import SVC
poly_kernel_svm_clf = Pipeline([
        ("scaler", StandardScaler()), # 缩放特征
        ("svm_clf", SVC(kernel="poly", degree=3, coef0=1, C=5)) # 多项式核SVC
    ])
poly_kernel_svm_clf.fit(X, y) # 训练数据
#+END_SRC

其中超参数coef0控制的是高阶还是低阶多项式影响的程度。

#+BEGIN_EXAMPLE
注意：
  1. 拟合不足时，应该提高多项式阶数;过拟合时，应该降低阶数。
  2. 多了解超参数的作用，可以帮助你快速筛选超参的有效范围，大大提高网格搜索的效率。
  3. 搜索时先进行一次粗略的网格搜索，再在最好的值附近进行下一轮更精细的搜索，这样会更高效。
#+END_EXAMPLE

*** 添加相似特征
解决非线性问题的另一个方法是添加相似特征。第一个实例的新特征由相似函数计算得出。如高斯RBF：

\phi_{\gamma}(*x*,l) = exp(-\gamma || *x*-l ||^{2})

它是一个钟形曲线。这个方法的缺点是：一个有m个实例，n个特征的训练集会被转换为一个有m个实例，
m个特征的训练集。如果训练集很大，将得到大量特征。

*** 高斯RBF核
与多项式特征法一样，相似特征法也 可以用于所有机器学习算法，但代价非常昂贵，尤其对大数据集
而言。同样可以使用 ~核技巧~, sklearn代码如下（与上面相比，只有kernel不同）：

#+BEGIN_SRC python
from sklearn.svm import SVC
rbf_kernel_svm_clf = Pipeline([
        ("scaler", StandardScaler()), # 缩放特征
        ("svm_clf", SVC(kernel="rbf", gamma=5, C=0.001)) # rbf核SVC
    ])
rbf_kernel_svm_clf.fit(X, y) # 训练数据
#+END_SRC

#+BEGIN_EXAMPLE
  1. 超参gamma的作用：增大gamma使钟形曲线变得更窄，每个实例的影响范围更小，决策边界更不规则;
     反之减小gamma会使决策边界更平坦。
  2. gamma就像一个正则化超参数，过拟合就减小它的值;拟合不足就增大它的值。
  2. 超参C的作用：正则化，越小正则化越强。
#+END_EXAMPLE

*** sklearn的分类算法对比

#+name: sklearn线性SVM分类算法对比
|--------------+----------------------------------------------------+----------------------------------+--------------------------------------------|
|              | LinearSVC类                                        | SVC类的linear kernel             | SGDClassifier类的hinge损失函数             |
|--------------+----------------------------------------------------+----------------------------------+--------------------------------------------|
| 调用方法     | LinearSVC(C=1, loss='hinge')                       | SVC(kernel='linear', C=1)        | SGDClassifier(loss='hinge', alpha=1/(m*C)) |
|--------------+----------------------------------------------------+----------------------------------+--------------------------------------------|
| 时间复杂度   | O(m*n)                                             | O(m^{2}*n)与O(m^{3}*n)之间       | O(m*n)                                     |
|--------------+----------------------------------------------------+----------------------------------+--------------------------------------------|
| 需要缩放？   | 是                                                 | 是                               | 是                                         |
|--------------+----------------------------------------------------+----------------------------------+--------------------------------------------|
| 支持核外？   | 否                                                 | 否                               | 是                                         |
|--------------+----------------------------------------------------+----------------------------------+--------------------------------------------|
| 运算速度     | 快                                                 | 慢                               | 慢                                         |
|--------------+----------------------------------------------------+----------------------------------+--------------------------------------------|
| 支持核技巧？ | 否                                                 | 是                               | 否                                         |
|--------------+----------------------------------------------------+----------------------------------+--------------------------------------------|
| 优点         | 基于 /liblinear/ 库实现的优化算法，收敛快          | 基于 /libsvm/ 库，支持核技巧     | 对大型数据集有效，对 ~在线分类~ 任务有效   |
|--------------+----------------------------------------------------+----------------------------------+--------------------------------------------|
| 缺点         | 不支持核技巧                                       | 只适用于复杂但中小型的训练集     | 不支持核技巧                               |
|--------------+----------------------------------------------------+----------------------------------+--------------------------------------------|
| 注意         | 要先减去平均值使训练集集中，还要设置超参loss和dual | 不要用在大型数据集上（超过十万） | 成本函数要可导才能用梯度下降               |
|--------------+----------------------------------------------------+----------------------------------+--------------------------------------------|

这么多核函数，该如何选择呢？

*经验：*
- 永远先从线性核函数开始尝试（记住：LinearSVC比SVC的linear kernel快得多），
  特别是当训练集很大或特征很多时。
- 如果训练不是很大，可以尝试高斯RBF核，大多数情况都很好用。
- 如果你还有多余时间和计算能力，可以使用交叉和网格搜索来尝试其他核函数，尤其是那些专门
  针对你的数据集数据结构的核函数（如字符串核）。
*** SVM回归（线性和非线性回归任务）
SVM不仅可以用于线性、非线性 ~分类~,还可以用于线性和非线性 ~回归~ 。需要转换一下思路：
不再是在拟合两个类别之间最宽的街道的同时限制间隔违例，SVM回归要做的是让尽可能多的实
例位于街道上，同时限制间隔违例（不在街道上的实例）。街道的宽度由超参数\epsilon
控制。

与SVM分类同理，间隔内添加更多的实例不影响SVM回归模型的预测，所以这个模型被为\epsilon
不敏感。

对于线性的回归任务，可以用sklearn的LinearSVR类来执行，代码如下：

#+BEGIN_SRC python
import numpy as np
from sklearn import datasets
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVR

iris = datasets.load_iris()
X = iris["data"][:, (2, 3)]  # petal length, petal width
y = (iris["target"] == 2).astype(np.float64)  # Iris-Virginica

svm_reg = Pipeline([
        ("scaler", StandardScaler()), # 需要缩放特征，并集中
        ("linear_svr", LinearSVR(epsilon=1.5)), # 线性支持向量机回归
    ])

svm_reg.fit(X, y)
svm_reg.predict([[5.5, 1.7]])
#+END_SRC

对于非线性的回归任务，可以用核化的SVC类：

#+BEGIN_SRC python
from sklearn.svm import SVR

svm_reg = Pipeline([
        ("scaler", StandardScaler()), # 需要缩放特征，并集中
        ("svr", SVR(kernel='poly',degree=2, C=100, epsilon=0.1)), # 多项式核支持向量机回归
    ])

svm_reg.fit(X, y)
#+END_SRC

#+BEGIN_EXAMPLE
注意：
1. SVR类是SVC类的回归等价物
2. LinearSVR类是LinearSVC类的回归等价物
3. 它们性质也与它们的等价物相同，见章节：sklearn的分类算法对比
#+END_EXAMPLE

** SVM的工作原理
上面我们讲了如何用sklearn来训练一个SVM分类器或回归器，但是SVM是如何预测的？
又是如何训练的呢？

#+BEGIN_EXAMPLE
注意：
1. 有个学习支持向量机的好地方，那就是sklearn库的帮助文档，https://scikit-learn.org/stable/modules/svm.html#
2. 你也许想再逛逛这个帮助文档的其他部分，可以让你获益非浅哦！ https://scikit-learn.org/stable/user_guide.html
#+END_EXAMPLE

*** 线性SVM分类
决策函数：

w^{T}\cdot *x* + b = w_{1}x_{1} + \cdot\cdot\cdot + w_{n}x_{n} + b

预测：如果上式结果为正，则预测为正类;为负则预测为负类。

软间隔SVM分类器的目标可以看成一个约束优化问题：

#+ATTR_HTML: :width 900
[[file:images/SVC2.png]]

\zeta^{(i)} 衡量的是第i个实例多在程度上允许间隔违例。

硬间隔和软间隔分类都是线性约束的凸二次优化问题，被称为二次规划（QP）。

可以对原始问题使用梯度下降，成本函数为 ~hinge损失函数~ ，使用方法与Loss回归一样。
原问题的成本函数为：

#+ATTR_HTML: :width 700
[[file:images/loss-hinge.png]]

#+BEGIN_EXAMPLE
使用对偶问题的原因是它可以使用核技巧，而原问题不可以。
#+END_EXAMPLE

*** 非线性SVM分类

#+BEGIN_QUOTE
Mercer定理：
  如果函数K(a, b)满足以下条件，则存在函数\phi，将 *a* *b* 映射到时另一空间，
  使得K(a, b) = \phi(*a*)^{T} \cdot \phi(*b*):
    1. K函数是连续的。
    2. K关于其自变量对称。
    3. 其他？
#+END_QUOTE

常用核函数：

#+ATTR_HTML: :width 800
[[file:images/kernels.png]]

对于大规模非线性问题，你可能需要使用神经网络模型。
*** 线性SVM回归

#+ATTR_HTML: :width 900
[[file:images/svr.png]]
* 第四章和第五章总结
今天总结了一下《机器学习实战》第四章和第五章的内容和sklearn的user guide相关
内容。通过算法之间的横向比较，对目前尝到的回归算法和分类算法有了更深刻地理解。但
总感觉对支持向量机的工作机制还是理解不够，书上内容过于简略，需要找其他的书或网页
来看看。

#+BEGIN_QUOTE
只读一本书不是学习，是娱乐。
#+END_QUOTE

不同的算法从不同的成本函数或损失函数出发，再加上不同的正则化项就形成了不同
算法。sklearn对这些算法都有实现，不同的实现适用的问题也不同，具体见表格。

** 回归算法
目前学习到的用于 ~回归任务~ 的算法包括：
- 线性回归（不加正则化的线性回归，或普通的最小二乘法，LinearRegression）
- 岭回归（Ridge）
- 套索回归（Lasso）
- 弹性网络（ElasticNet）
- 线性支持向量机回归器（LinearSVR）
- 支持向量机回归器（SVR）
- 随机梯度下降回归器（SGDRegressor）

sklearn实现的不同回归算法的横向对比见下表：

| sklearn类名      | 损失函数(loss)              | 正则项(penalty)                        | 模型名称           | 是否支持核技巧 | 优点                                                                                 | 缺点                                    |
|------------------+-----------------------------+----------------------------------------+--------------------+----------------+--------------------------------------------------------------------------------------+-----------------------------------------|
| LinearRegression | squared_loss                | 无                                     | 普通最小二乘       | 否             | 使用 *X* 矩阵的SVD分解（闭式解），直接求解，不用迭代                                 | 大数据集太慢，特征相关性强时误差很大    |
| Ridge            | squared_loss                | l_{2}正则项                            | 岭回归             | 否             | 闭式解法，同上;比普通最小二乘法稳定                                                  | 大数据集太慢                            |
| Lasso            | squared_loss                | l_{1}正则项                            | 套索回归           | 否             | 实现方法：坐标下降（迭代优化）;可用于特征选择                                        | 当n>m或特征强相关时可能非常不稳定       |
| ElasticNet       | squared_loss                | \rho l_{1}+(1-\rho)l_{2}               | 弹性网络           | 否             | 继承了Ridge的稳定性，也可作特征选择                                                  | 大数据集太慢                            |
| SGDRegressor     | squared_loss                | {None, 'l_{2}', 'l_{1}', 'elasticnet'} | 以上四种算法       | 否             | 随机梯度下降（迭代优化），可用于大数据集（高效）                                     | 对缩放敏感，需要设置一些超参,对缩放敏感 |
|                  | huber                       | {None, 'l_{2}', 'l_{1}', 'elasticnet'} | Huber回归          | 否             | 同上？                                                                               | 同上？                                  |
|                  | epsilon_insensitive         | {None, 'l_{2}', 'l_{1}', 'elasticnet'} | 线性支持向量机回归 | 否             | 同上？                                                                               | 同上？                                  |
|                  | squared_epsilon_insensitive | {None, 'l_{2}', 'l_{1}', 'elasticnet'} | 无                 | 否             | 同上？                                                                               | 同上？                                  |
| LinearSVR        | epsilon_insensitive         | l_{1}正则项                            | 线性支持向量机回归 | 否             | 基于liblinear的优化算法，快速收敛 （迭代优化），在高维空间十分高效                   | 要小心过拟合，不提供概率估计,对缩放敏感 |
|                  | squared_epsilon_insensitive | l_{2}正则项                            | 线性支持向量机回归 | 否             | 同上？                                                                               | 同上？                                  |
| SVR              | epsilon_insensitive         | l_{2}正则项                            | 支持向量机回归     | 是             | kernel={linear, rbf, poly ,sigmoid ,precomputed ,自定义}可以用于线性、非线性回归任务 | 只适用于复杂但中小型的训练集,对缩放敏感 |

** 分类算法
目前学习到的用于 ~分类任务~ 的算法包括：
- 逻辑回归（LogisticRegression）
- 多元逻辑回归（Softmax回归）
- 线性支持向量机分类器（LinearSVC）
- 支持向量机分类器（SVC）
- 随机梯度下降分类器（SGDClassifier）

| sklearn类名        | 损失函数(loss)                                                  | 正则项(penalty)                        | 模型名称           | 优点                                                                                  | 缺点                                    |
|--------------------+-----------------------------------------------------------------+----------------------------------------+--------------------+---------------------------------------------------------------------------------------+-----------------------------------------|
| LogisticRegression | log                                                             | {None, 'l_{2}', 'l_{1}', 'elasticnet'} | 逻辑回归           | 求解器solver={newton-cg, lbfas, liblinear, sag, saga}, 可以实现多分类、多项式逻辑回归 | ？                                      |
| SGDClassifier      | hinge                                                           | {None, 'l_{2}', 'l_{1}', 'elasticnet'} | 线性支持向量机分类 | 高效，对大数据集有效                                                                  | 对缩放敏感                              |
|                    | log                                                             | 同上                                   | 逻辑回归           | 同上？                                                                                | 同上？                                  |
|                    | {modified_huber, squared_hinge, perceptron, 所有回归的损失函数} | 同上                                   | ？                 | 同上？                                                                                | 同上？                                  |
| LinearSVC          | {hinge, squared_hinge}                                          | {l_{2}, l_{1}}                         | 线性支持向量机分类 | 基于liblinear的优化算法，快速收敛 （迭代优化）                                        | 不支持核技巧,对缩放敏感                 |
| SVC                | hinge                                                           | l_{2}                                  | 支持向量机分类     | 支持核技术kernel={linear, rbf, poly, sigmoid, percomputed, 自定义}                    | 只适用于复杂但中小型的训练集,对缩放敏感 |

** 这么多算法该如何选择呢？
*经验：*
- 通常而言，有正则化总比没有强，所以大多数时候应该避免使用纯线性回归。岭回归是个不错的默认选择，
  但如果你觉得实际用到的特征只有少数几个，那就应该更倾向于Lasso或弹性网络，因为它们可以对特征进行自动选择。
  一般而言，弹性网络优于Lasso回归，因为当特征数大于训练实例数或特征强相关时，Lasso回归可能非常不稳定。
- 永远先从线性核函数开始尝试（记住：LinearSVC比SVC的linear kernel快得多），
  特别是当训练集很大或特征很多时。
- 如果训练不是很大，可以尝试高斯RBF核，大多数情况都很好用。
- 如果你还有多余时间和计算能力，可以使用交叉和网格搜索来尝试其他核函数，尤其是那些专门
  针对你的数据集数据结构的核函数（如字符串核）。
