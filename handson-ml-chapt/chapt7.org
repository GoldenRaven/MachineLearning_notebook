* 第七章 集成学习和随机森林
集体智慧：随机地向几千人问一个复杂问题，然后汇总他们的回答，这个汇总回答往往比专家
的回答要好。

同样，聚合一组预测器（可以是回归、分类等等）其结果也比最好的单个预测器要好，
这样的一组预测器，称为集成，这种技术也称为 ~集成学习~ 。聚合的方法也多种多样。

~随机森林~ 是决策树算法的集成，它是迄今可用的最强大的机器学习方法之一。

事实上，在机器学习竞赛中获胜的方案通常都涉及多种集成方法。（如http://netflixprize.com/）
本章将学习以下几种最流行的集成方法： ~bagging~, ~boosting~, ~stacking~,
还会学习 ~随机森林~.
** 投票分类器（不同算法在相同训练集上的聚合）
集成方法可以将弱学习器（仅比随机猜测好一点）变成一个强学习器（高准确率），
只要弱学习器的种类和数量足够多即可。

*为什么呢？原因见P167,但我还是没看懂？？？*
大数定理：随着不断地投掷硬币，正面朝上的概率会越来越接近于正面的概率。

如果每个分类器只有51%的正确率，当以大多数投票的类别作为预测结果时，可以得到75%的准确率。
但是前提是：所有分类器都是完全独立的，彼此的错误毫不相关。这是不可能的，因为它们在相同的
数据上训练得到，很可能犯相同的错误。

#+BEGIN_EXAMPLE
注意：当预测器尽可能互相独立时，集成方法的效果最优。使用不同的算法训练可以增加它们犯不同类型错误的机会，从而提升集成的准确率。
#+END_EXAMPLE

sklearn中实现投票分类器的代码如下：

#+BEGIN_SRC python
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier

tree_clf = DecisionTreeClassifier() # 调用决策树
log_clf = LogisticRegression() # 调用逻辑回归
svm_clf = SVC() # 调用支持向量机分类

voting_clf = VotingClassifier( # 组装投票分类器
    estimators=[('lr', log_clf), ('dt', tree_clf), ('svc', svm_clf)],
    voting='hard' # 投票方法：硬投票
)
voting_clf.fit(X_train, y_train)
#+END_SRC

*** 硬投票分类器
硬投票：聚合每个分类器的预测，将得票最多的类别作为预测结果。
sklearn实现：参数 ~volting='soft'~.
*** 软投票分类器
软投票：如果所有被集成的分类器都能给出类别概率（即有predict_proba()方法），那到么可以将不同类别
的概率在所有分类器上进行平均，以平均概率最高的类别作为预测结果。

通常来说，软投票比硬投票表现更好！因为它给予那些高度自信的投票更高的权重。

sklearn实现：1.确保所有预测器都能给出概率。2.参数 ~volting='soft'~.

#+BEGIN_EXAMPLE
注意：SVC默认不能给出概率，所以不能用以软投票;除非设probability为true,但这会开启交叉验证，减慢训练速度。
#+END_EXAMPLE

** bagging/pasting
