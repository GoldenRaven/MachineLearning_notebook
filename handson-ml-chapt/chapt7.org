* 第七章 集成学习和随机森林
集体智慧：随机地向几千人问一个复杂问题，然后汇总他们的回答，这个汇总回答往往比专家
的回答要好。

同样，聚合一组预测器（可以是回归、分类等等）其结果也比最好的单个预测器要好，
这样的一组预测器，称为集成，这种技术也称为 ~集成学习~ 。聚合的方法也多种多样。

~随机森林~ 是决策树算法的集成，它是迄今可用的最强大的机器学习方法之一。

事实上，在机器学习竞赛中获胜的方案通常都涉及多种集成方法。（如http://netflixprize.com/）
本章将学习以下几种最流行的集成方法： ~bagging~, ~boosting~, ~stacking~,
还会学习 ~随机森林~.
** 投票分类器（不同算法在相同训练集上的聚合）
集成方法可以将弱学习器（仅比随机猜测好一点）变成一个强学习器（高准确率），
只要弱学习器的种类和数量足够多即可。 *为什么呢？原因见P167,但我还是没看懂？？？*

大数定理：随着不断地投掷硬币，正面朝上的概率会越来越接近于正面的概率。

如果每个分类器只有51%的正确率，当以大多数投票的类别作为预测结果时，可以得到75%的准确率。
但是前提是：所有分类器都是完全独立的，彼此的错误毫不相关。这是不可能的，因为它们在相同的
数据上训练得到，很可能犯相同的错误。

#+BEGIN_EXAMPLE
注意：当预测器尽可能互相独立时，集成方法的效果最优。使用不同的算法训练可以增加它们犯不同类型错误的机会，从而提升集成的准确率。
#+END_EXAMPLE

*** 硬投票分类器
硬投票：聚合每个分类器的预测，将得票最多的类别作为预测结果。

*sklearn实现* ：参数 ~volting='soft'~
*** 软投票分类器
软投票：如果所有被集成的分类器都能给出类别概率（即有predict_proba()方法），那到么可以将不同类别
的概率在所有分类器上进行平均，以平均概率最高的类别作为预测结果。

通常来说，软投票比硬投票表现更好！因为它给予那些高度自信的投票更高的权重。

*sklearn实现* ：1.确保所有预测器都能给出概率。2.参数 ~volting='soft'~

#+BEGIN_EXAMPLE
注意：SVC默认不能给出概率，所以不能用以软投票;除非设probability为true,但这会开启交叉验证，减慢训练速度。
#+END_EXAMPLE

*** sklearn中投票分类器（回归器）的实现

#+BEGIN_SRC python
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier

tree_clf = DecisionTreeClassifier() # 调用决策树
log_clf = LogisticRegression() # 调用逻辑回归
svm_clf = SVC() # 调用支持向量机分类

voting_clf = VotingClassifier( # 组装投票分类器
    estimators=[('lr', log_clf), ('dt', tree_clf), ('svc', svm_clf)],
    voting='hard' # 投票方法：硬投票
)
voting_clf.fit(X_train, y_train)
#+END_SRC

~VotingClassifer~ 类中还有个有用的参数 ~weights~ ,用以加权计算在 ~硬投票~ 时的分类结果或
~软投票~ 时平均之前的类别概率。比如 ~weights=[1, 2, 3]~ 或 ~weights=[0.2, 0.2, 0.3]~
元素个数要等于 ~n_classifiers~ 。

与分类相对应的回归实现为 ~VotingRegressor~ ，使用方法也相同。

** 三种不同的集成方法
*** bagging/pasting（相同算法在不同训练集随机子集上的聚合）
bagging是bootstrap aggregating的缩写，即自举汇聚法。bootstrap的意思是adj.依靠自已力量的，
boot的意思是靴子，strap的意思是（皮）带子。aggregate是v. 合计、聚合的意思。
*** sklearn中bagging/pasting的实现
*** boosting 提升法（将弱学习结合成强学习的任意集成方法）
大多数提升法的总体思路是先循环训练预测器，每一次都对前序做出一些修正。有许多提升法，
比较流行的是自适应提升Adaboost和梯度提升Gradient Boosting.
**** AdaBoost 自适应提升
对前序修正的方法之一，就是更多地关注前序拟合不足的训练实例，从而使新的预测器不断地越来越专注于难缠
的问题，这就是Adaboost使用的技术。

#+BEGIN_EXAMPLE
疑问：训练时就更加关注难预测的实例了？还是说，只为了得到聚合权重，
     训练时前序预测器的预测结果不影响后序预测器的预测？
#+END_EXAMPLE
**** sklearn中Adaboost的实现
**** Gradient Boosting 梯度提升
**** sklearn中Gradient boosting的实现
*** stacking 堆叠法
*** sklearn对stacking的实现（不直接支持！）
